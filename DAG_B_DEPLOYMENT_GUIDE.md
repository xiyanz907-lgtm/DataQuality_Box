# DAG B éƒ¨ç½²ä¸æµ‹è¯•æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [åŠŸèƒ½æ¦‚è¿°](#åŠŸèƒ½æ¦‚è¿°)
2. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
3. [å‰ç½®æ¡ä»¶](#å‰ç½®æ¡ä»¶)
4. [éƒ¨ç½²æ­¥éª¤](#éƒ¨ç½²æ­¥éª¤)
5. [æµ‹è¯•éªŒè¯](#æµ‹è¯•éªŒè¯)
6. [ç›‘æ§ä¸å‘Šè­¦](#ç›‘æ§ä¸å‘Šè­¦)
7. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
8. [è¿ç»´æ‰‹å†Œ](#è¿ç»´æ‰‹å†Œ)

---

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

**DAG B (Asset Packing DAG)** æ˜¯æ•°æ®æ²»ç†å¹³å°çš„èµ„äº§æ‰“åŒ…æ¨¡å—ï¼Œè´Ÿè´£å°† DAG A è¯†åˆ«çš„ P1 èµ„äº§ï¼ˆé«˜ä»·å€¼åœºæ™¯æ•°æ®ï¼‰æ‰“åŒ…å¹¶ä¸Šä¼ åˆ°å­˜å‚¨ç³»ç»Ÿã€‚

### æ ¸å¿ƒç‰¹æ€§
- âœ… **äº‹ä»¶é©±åŠ¨**ï¼šç”± DAG A é€šè¿‡ Airflow Dataset è‡ªåŠ¨è§¦å‘
- âœ… **å¼‚æ­¥å¤„ç†**ï¼šæ”¯æŒå¼‚æ­¥æ‰“åŒ…æ¥å£ + è½®è¯¢æœºåˆ¶
- âœ… **åƒµå°¸ä»»åŠ¡å¤„ç†**ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶é‡ç½®è¶…æ—¶ä»»åŠ¡
- âœ… **é‡è¯•æœºåˆ¶**ï¼šå¤±è´¥è‡ªåŠ¨é‡è¯• 3 æ¬¡ï¼Œè¶…è¿‡åˆ™æ°¸ä¹…æ”¾å¼ƒ
- âœ… **æ‰¹é‡å¤„ç†**ï¼šæ¯æ¬¡æœ€å¤šå¤„ç† 50 æ¡èµ„äº§
- âœ… **è¡Œé”é˜²å¹¶å‘**ï¼šä½¿ç”¨ `FOR UPDATE SKIP LOCKED` é˜²æ­¢å†²çª

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ•°æ®æµè½¬
```
DAG A (è¯†åˆ«èµ„äº§) 
    â†“ å†™å…¥é˜Ÿåˆ— 
    â†“ è§¦å‘ Dataset
DAG B (æ‰“åŒ…èµ„äº§)
    â”œâ”€ æ¸…ç†åƒµå°¸ä»»åŠ¡
    â”œâ”€ è·å–å¾…å¤„ç†èµ„äº§ (PENDING)
    â”œâ”€ è°ƒç”¨æ‰“åŒ…æœåŠ¡ (å¼‚æ­¥)
    â”œâ”€ è½®è¯¢æ‰“åŒ…çŠ¶æ€ (æœ€å¤š 60 æ¬¡)
    â”œâ”€ æ›´æ–°å…ƒæ•°æ®è¡¨ (process_status = PACKAGED)
    â””â”€ å‘é€å¤±è´¥æ±‡æ€»é‚®ä»¶
```

### çŠ¶æ€æœº
```
PENDING â†’ PROCESSING â†’ POLLING â†’ SUCCESS
                     â†“          â†“
                   FAILED â† ABANDONED
```

- **PENDING**: å¾…å¤„ç†ï¼ˆåˆå§‹çŠ¶æ€ï¼‰
- **PROCESSING**: æ‰“åŒ…ä¸­ï¼ˆå·²è°ƒç”¨æ‰“åŒ…æ¥å£ï¼‰
- **POLLING**: è½®è¯¢ä¸­ï¼ˆç­‰å¾…æ‰“åŒ…å®Œæˆï¼‰
- **SUCCESS**: æˆåŠŸ
- **FAILED**: å¤±è´¥ï¼ˆå¯é‡è¯•ï¼Œ`retry_count < 3`ï¼‰
- **ABANDONED**: å·²æ”¾å¼ƒï¼ˆ`retry_count >= 3`ï¼‰

---

## âœ… å‰ç½®æ¡ä»¶

### 1. æ•°æ®åº“è¡¨
éœ€è¦åˆ›å»ºä»¥ä¸‹è¡¨ï¼š

```bash
# 1. åˆ›å»ºæ‰“åŒ…é˜Ÿåˆ—è¡¨
mysql -u root -p < database/schemas/schema_governance_asset_packing_queue.sql

# 2. å‡çº§å…ƒæ•°æ®è¡¨
mysql -u root -p < database/schemas/schema_auto_test_case_catalog_v2_migration.sql
```

### 2. æ‰“åŒ…æœåŠ¡
ç¡®è®¤æ‰“åŒ…æœåŠ¡ API å¯è®¿é—®ï¼š

```bash
# æµ‹è¯•æ‰“åŒ…æ¥å£
curl -X POST https://mock.apipost.net/mock/34a21a/api/launcher/queryInfluxData \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "startTime": "2026-01-01T10:00:00Z",
    "endTime": "2026-01-01T12:00:00Z",
    "vehicleId": "V001",
    "basePath": "/data/assets/"
  }'

# æµ‹è¯•æŸ¥è¯¢æ¥å£
curl -X GET "https://mock.apipost.net/mock/34a21a/api/launcher/querySyncCacheResult?key=YOUR_PACK_KEY" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

### 3. Airflow ç‰ˆæœ¬
- **Airflow >= 2.4.0** (æ”¯æŒ Dataset åŠŸèƒ½)

æ£€æŸ¥ç‰ˆæœ¬ï¼š
```bash
docker exec deploy-airflow-1 airflow version
```

### 4. MySQL è¿æ¥
ç¡®è®¤ Airflow ä¸­å·²é…ç½® `datalog_mysql_conn` è¿æ¥ï¼š

```bash
# æŸ¥çœ‹è¿æ¥åˆ—è¡¨
docker exec deploy-airflow-1 airflow connections list | grep datalog_mysql_conn
```

---

## ğŸš€ éƒ¨ç½²æ­¥éª¤

### Step 1: æ‹‰å–æœ€æ–°ä»£ç 
```bash
cd /home/ubuntu/cactus_box/cactus-box
git pull origin main
```

### Step 2: æ£€æŸ¥æ–‡ä»¶æ¸…å•
ç¡®è®¤ä»¥ä¸‹æ–‡ä»¶å·²å­˜åœ¨ï¼š

```bash
# æ ¸å¿ƒæ–‡ä»¶
âœ… plugins/datasets.py                                      # Dataset å®šä¹‰
âœ… plugins/services/packing_service.py                      # æ‰“åŒ…æœåŠ¡å®¢æˆ·ç«¯
âœ… dags/asset_packing_dag.py                                # DAG B
âœ… dags/governance_main_dag.py                              # DAG A (å·²ä¿®æ”¹)
âœ… plugins/domian/context.py                                # AssetItem æ·»åŠ  rule_id
âœ… plugins/operators/aggregator.py                          # add_asset æ·»åŠ  rule_id

# æ•°æ®åº“æ–‡ä»¶
âœ… database/schemas/schema_governance_asset_packing_queue.sql
âœ… database/schemas/schema_auto_test_case_catalog_v2_migration.sql
âœ… database/monitoring/asset_packing_monitor.sql
```

### Step 3: åˆ›å»ºæ•°æ®åº“è¡¨
```bash
# è¿æ¥åˆ° MySQL
docker exec -it deploy-mysql-1 mysql -u root -p

# åˆ‡æ¢æ•°æ®åº“
USE your_database_name;

# æ‰§è¡Œå»ºè¡¨ SQL
SOURCE /path/to/schema_governance_asset_packing_queue.sql;
SOURCE /path/to/schema_auto_test_case_catalog_v2_migration.sql;

# éªŒè¯è¡¨ç»“æ„
DESC governance_asset_packing_queue;
DESC auto_test_case_catalog;
```

### Step 4: é‡å¯ Airflow
```bash
cd /home/ubuntu/cactus_box/cactus-box/deploy
docker-compose restart airflow
```

### Step 5: æ£€æŸ¥ DAG çŠ¶æ€
```bash
# æŸ¥çœ‹ DAG åˆ—è¡¨
docker exec deploy-airflow-1 airflow dags list | grep -E "(governance_main_dag|asset_packing_dag)"

# æŸ¥çœ‹ Dataset
docker exec deploy-airflow-1 airflow datasets list
```

é¢„æœŸè¾“å‡ºï¼š
```
âœ… governance_main_dag (Active)
âœ… asset_packing_dag (Active, Schedule: Dataset)
```

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### Test 1: æ‰‹åŠ¨è§¦å‘ DAG A
```bash
# è§¦å‘ DAG A
docker exec deploy-airflow-1 airflow dags trigger governance_main_dag

# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
docker exec deploy-airflow-1 airflow dags state governance_main_dag
```

### Test 2: éªŒè¯é˜Ÿåˆ—å†™å…¥
```sql
-- æŸ¥çœ‹é˜Ÿåˆ—ä¸­çš„èµ„äº§
SELECT * FROM governance_asset_packing_queue 
WHERE batch_id = 'BATCH_XXXXXX'
ORDER BY created_at DESC;

-- é¢„æœŸç»“æœï¼š
-- å¦‚æœ DAG A è¯†åˆ«åˆ° P1 èµ„äº§ï¼Œé˜Ÿåˆ—ä¸­åº”è¯¥æœ‰å¯¹åº”è®°å½•
-- status = 'PENDING'
```

### Test 3: éªŒè¯ Dataset è§¦å‘
```bash
# æŸ¥çœ‹ Dataset å†å²
docker exec deploy-airflow-1 airflow datasets list-events

# é¢„æœŸï¼šåº”è¯¥æœ‰ GOVERNANCE_ASSET_DATASET çš„è§¦å‘è®°å½•
```

### Test 4: éªŒè¯ DAG B è‡ªåŠ¨è§¦å‘
```bash
# æŸ¥çœ‹ DAG B è¿è¡Œå†å²
docker exec deploy-airflow-1 airflow dags list-runs -d asset_packing_dag

# é¢„æœŸï¼šDAG B åº”è¯¥åœ¨ DAG A å®Œæˆåè‡ªåŠ¨è§¦å‘
```

### Test 5: éªŒè¯æ‰“åŒ…ç»“æœ
```sql
-- æŸ¥çœ‹æ‰“åŒ…æˆåŠŸçš„èµ„äº§
SELECT * FROM governance_asset_packing_queue 
WHERE status = 'SUCCESS'
ORDER BY processed_at DESC
LIMIT 10;

-- æŸ¥çœ‹å…ƒæ•°æ®è¡¨æ›´æ–°
SELECT * FROM auto_test_case_catalog
WHERE process_status = 'PACKAGED'
ORDER BY updated_at DESC
LIMIT 10;
```

### Test 6: æµ‹è¯•åƒµå°¸ä»»åŠ¡å¤„ç†
```sql
-- æ‰‹åŠ¨æ¨¡æ‹Ÿåƒµå°¸ä»»åŠ¡
UPDATE governance_asset_packing_queue
SET status = 'PROCESSING',
    updated_at = NOW() - INTERVAL 3 HOUR
WHERE id = <æŸä¸ªID>;

-- è§¦å‘ DAG Bï¼ˆæˆ–ç­‰å¾…è‡ªåŠ¨è§¦å‘ï¼‰
-- éªŒè¯åƒµå°¸ä»»åŠ¡è¢«é‡ç½®
SELECT * FROM governance_asset_packing_queue WHERE id = <æŸä¸ªID>;
-- é¢„æœŸï¼šstatus åº”è¯¥å˜ä¸º 'PENDING'ï¼Œretry_count +1
```

### Test 7: æµ‹è¯•é‡è¯•æ¬¡æ•°é™åˆ¶
```sql
-- æ¨¡æ‹Ÿè¶…è¿‡é‡è¯•æ¬¡æ•°çš„ä»»åŠ¡
UPDATE governance_asset_packing_queue
SET status = 'PROCESSING',
    retry_count = 3,
    updated_at = NOW() - INTERVAL 3 HOUR
WHERE id = <æŸä¸ªID>;

-- è§¦å‘ DAG B
-- éªŒè¯ä»»åŠ¡è¢«æ ‡è®°ä¸º ABANDONED
SELECT * FROM governance_asset_packing_queue WHERE id = <æŸä¸ªID>;
-- é¢„æœŸï¼šstatus = 'ABANDONED'
```

---

## ğŸ“Š ç›‘æ§ä¸å‘Šè­¦

### 1. é˜Ÿåˆ—ç§¯å‹ç›‘æ§
```sql
-- æ¯åˆ†é’Ÿæ‰§è¡Œ
SELECT COUNT(*) AS pending_count
FROM governance_asset_packing_queue
WHERE status = 'PENDING';

-- å‘Šè­¦é˜ˆå€¼ï¼špending_count > 500
```

### 2. åƒµå°¸ä»»åŠ¡ç›‘æ§
```sql
-- æ¯ 10 åˆ†é’Ÿæ‰§è¡Œ
SELECT COUNT(*) AS zombie_count
FROM governance_asset_packing_queue
WHERE status IN ('PROCESSING', 'POLLING')
  AND updated_at < NOW() - INTERVAL 2 HOUR;

-- å‘Šè­¦é˜ˆå€¼ï¼šzombie_count > 0
```

### 3. æ‰“åŒ…æˆåŠŸç‡ç›‘æ§
```sql
-- æ¯ 15 åˆ†é’Ÿæ‰§è¡Œ
SELECT 
    ROUND(
        SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) * 100.0 / COUNT(*),
        2
    ) AS success_rate_pct
FROM governance_asset_packing_queue
WHERE processed_at >= NOW() - INTERVAL 1 HOUR;

-- å‘Šè­¦é˜ˆå€¼ï¼šsuccess_rate_pct < 80%
```

### 4. å¤„ç†è€—æ—¶ç›‘æ§
```sql
-- æ¯ 30 åˆ†é’Ÿæ‰§è¡Œ
SELECT 
    AVG(TIMESTAMPDIFF(SECOND, pack_started_at, processed_at)) AS avg_duration_sec,
    MAX(TIMESTAMPDIFF(SECOND, pack_started_at, processed_at)) AS max_duration_sec
FROM governance_asset_packing_queue
WHERE status = 'SUCCESS'
  AND processed_at >= NOW() - INTERVAL 1 HOUR;

-- å‘Šè­¦é˜ˆå€¼ï¼šmax_duration_sec > 600 (10åˆ†é’Ÿ)
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šDAG B æœªè‡ªåŠ¨è§¦å‘

**ç—‡çŠ¶**ï¼šDAG A å®Œæˆåï¼ŒDAG B æ²¡æœ‰å¯åŠ¨

**æ’æŸ¥æ­¥éª¤**ï¼š
```bash
# 1. æ£€æŸ¥ Dataset æ˜¯å¦æ­£ç¡®å®šä¹‰
docker exec deploy-airflow-1 airflow datasets list | grep governance_asset_packing_queue

# 2. æ£€æŸ¥ DAG A æ˜¯å¦å£°æ˜äº† outlets
docker exec deploy-airflow-1 python3 -c "
from dags.governance_main_dag import dag
print(dag.tasks[-1].outlets)
"

# 3. æ£€æŸ¥ DAG B æ˜¯å¦ç›‘å¬äº† schedule
docker exec deploy-airflow-1 python3 -c "
from dags.asset_packing_dag import dag
print(dag.schedule_interval)
"

# 4. æŸ¥çœ‹ Dataset è§¦å‘å†å²
docker exec deploy-airflow-1 airflow datasets list-events
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç¡®è®¤ `plugins/datasets.py` è¢«æ­£ç¡®åŠ è½½
- ç¡®è®¤ DAG A çš„ `save_assets_task` æœ‰ `outlets=[GOVERNANCE_ASSET_DATASET]`
- ç¡®è®¤ DAG B çš„ `schedule=[GOVERNANCE_ASSET_DATASET]`

### é—®é¢˜ 2ï¼šæ‰“åŒ…æœåŠ¡è°ƒç”¨å¤±è´¥

**ç—‡çŠ¶**ï¼šæ‰€æœ‰èµ„äº§éƒ½æ ‡è®°ä¸º FAILEDï¼Œé”™è¯¯ä¿¡æ¯æ˜¾ç¤º HTTP é”™è¯¯

**æ’æŸ¥æ­¥éª¤**ï¼š
```bash
# 1. æ‰‹åŠ¨æµ‹è¯•æ‰“åŒ…æ¥å£
curl -X POST https://mock.apipost.net/mock/34a21a/api/launcher/queryInfluxData \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"startTime": "2026-01-01T10:00:00Z", "endTime": "2026-01-01T12:00:00Z", "vehicleId": "V001", "basePath": "/data/"}'

# 2. æ£€æŸ¥ Airflow æ—¥å¿—
docker logs deploy-airflow-1 | grep "PackingServiceClient"
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥æ‰“åŒ…æœåŠ¡æ˜¯å¦åœ¨åŒä¸€ç½‘ç»œ
- æ£€æŸ¥ Authorization Token æ˜¯å¦è¿‡æœŸ
- ä¿®æ”¹ `plugins/services/packing_service.py` ä¸­çš„ `BASE_URL` å’Œ `AUTH_TOKEN`

### é—®é¢˜ 3ï¼šåƒµå°¸ä»»åŠ¡æœªè¢«æ¸…ç†

**ç—‡çŠ¶**ï¼šPROCESSING çŠ¶æ€çš„ä»»åŠ¡è¶…è¿‡ 2 å°æ—¶ä»æœªé‡ç½®

**æ’æŸ¥æ­¥éª¤**ï¼š
```sql
-- 1. æŸ¥è¯¢åƒµå°¸ä»»åŠ¡
SELECT * FROM governance_asset_packing_queue
WHERE status IN ('PROCESSING', 'POLLING')
  AND updated_at < NOW() - INTERVAL 2 HOUR;

-- 2. æ£€æŸ¥ cleanup_zombie_tasks æ˜¯å¦æ‰§è¡Œ
-- æŸ¥çœ‹ Airflow æ—¥å¿—
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ‰‹åŠ¨è§¦å‘ DAG B
- æ£€æŸ¥ `cleanup_zombie_tasks` ä»»åŠ¡çš„æ‰§è¡Œæ—¥å¿—
- å¦‚æœå¿…è¦ï¼Œæ‰‹åŠ¨é‡ç½®ï¼š
```sql
UPDATE governance_asset_packing_queue
SET status = 'PENDING',
    retry_count = retry_count + 1
WHERE id IN (...);
```

### é—®é¢˜ 4ï¼šå…ƒæ•°æ®è¡¨æœªæ›´æ–°

**ç—‡çŠ¶**ï¼šé˜Ÿåˆ—ä¸­æ˜¾ç¤º SUCCESSï¼Œä½† `auto_test_case_catalog` è¡¨çš„ `process_status` ä»æ˜¯ `IDENTIFIED`

**æ’æŸ¥æ­¥éª¤**ï¼š
```bash
# 1. æ£€æŸ¥ update_metadata ä»»åŠ¡æ—¥å¿—
docker exec deploy-airflow-1 airflow tasks logs asset_packing_dag update_metadata <execution_date>

# 2. æ‰‹åŠ¨æµ‹è¯•æ›´æ–°é€»è¾‘
docker exec -it deploy-mysql-1 mysql -u root -p -e "
UPDATE auto_test_case_catalog
SET process_status = 'PACKAGED'
WHERE id IN (
    SELECT asset_id FROM governance_asset_packing_queue WHERE status = 'SUCCESS'
);
"
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç¡®è®¤ `auto_test_case_catalog` è¡¨ä¸­æœ‰å¯¹åº”çš„ `id` è®°å½•
- æ£€æŸ¥ asset_id æ˜¯å¦æ­£ç¡®æ˜ å°„åˆ° meta è¡¨çš„ä¸»é”®

---

## ğŸ› ï¸ è¿ç»´æ‰‹å†Œ

### æ‰‹åŠ¨é‡è¯•å¤±è´¥ä»»åŠ¡
```sql
-- å°† FAILED ä»»åŠ¡é‡ç½®ä¸º PENDINGï¼ˆä¼šåœ¨ä¸‹æ¬¡ DAG B è§¦å‘æ—¶é‡è¯•ï¼‰
UPDATE governance_asset_packing_queue
SET status = 'PENDING',
    error_message = NULL,
    updated_at = NOW()
WHERE status = 'FAILED'
  AND retry_count < 3;
```

### æ‰‹åŠ¨æ”¾å¼ƒä»»åŠ¡
```sql
-- å°†æŸä¸ªä»»åŠ¡æ ‡è®°ä¸º ABANDONEDï¼ˆä¸å†é‡è¯•ï¼‰
UPDATE governance_asset_packing_queue
SET status = 'ABANDONED',
    error_message = 'Manually abandoned by admin',
    updated_at = NOW()
WHERE id = <ä»»åŠ¡ID>;
```

### æŸ¥çœ‹é˜Ÿåˆ—ç»Ÿè®¡
```sql
SELECT 
    status,
    COUNT(*) AS count
FROM governance_asset_packing_queue
GROUP BY status;
```

### æ¸…ç†å†å²æ•°æ®ï¼ˆä¿ç•™ 7 å¤©ï¼‰
```sql
DELETE FROM governance_asset_packing_queue
WHERE created_at < NOW() - INTERVAL 7 DAY;
```

### æ‰‹åŠ¨è§¦å‘ DAG B
```bash
docker exec deploy-airflow-1 airflow dags trigger asset_packing_dag
```

---

## ğŸ“ è”ç³»æ–¹å¼
- **Owner**: data-governance@example.com
- **Slack**: #data-governance-alerts
- **On-Call**: æŸ¥çœ‹ PagerDuty æ’ç­è¡¨

---

## âœ… éƒ¨ç½²æ¸…å•

éƒ¨ç½²å‰è¯·ç¡®è®¤ï¼š

- [ ] æ•°æ®åº“è¡¨å·²åˆ›å»ºï¼ˆ`governance_asset_packing_queue` + `auto_test_case_catalog` å‡çº§ï¼‰
- [ ] æ‰“åŒ…æœåŠ¡ API å¯è®¿é—®
- [ ] Airflow ç‰ˆæœ¬ >= 2.4.0
- [ ] MySQL è¿æ¥ `datalog_mysql_conn` å·²é…ç½®
- [ ] DAG A å’Œ DAG B åœ¨ Airflow UI ä¸­æ˜¾ç¤ºä¸º Active
- [ ] Dataset `GOVERNANCE_ASSET_DATASET` åœ¨ Airflow UI ä¸­å¯è§
- [ ] æ‰‹åŠ¨è§¦å‘ DAG A æµ‹è¯•é€šè¿‡
- [ ] é˜Ÿåˆ—å†™å…¥æµ‹è¯•é€šè¿‡
- [ ] DAG B è‡ªåŠ¨è§¦å‘æµ‹è¯•é€šè¿‡
- [ ] æ‰“åŒ…ç»“æœéªŒè¯é€šè¿‡
- [ ] åƒµå°¸ä»»åŠ¡å¤„ç†æµ‹è¯•é€šè¿‡
- [ ] ç›‘æ§ SQL å·²é›†æˆåˆ°ç›‘æ§ç³»ç»Ÿ
- [ ] å‘Šè­¦è§„åˆ™å·²é…ç½®

---

**éƒ¨ç½²æ—¥æœŸ**: 2026-02-02  
**ç‰ˆæœ¬**: v1.0  
**æ–‡æ¡£ç»´æŠ¤**: Data Governance Team
