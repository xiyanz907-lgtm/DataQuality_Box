"""
infra/operators.py
æ²»ç†ç®—å­åŸºç±»

æä¾›ç»Ÿä¸€çš„æ‰§è¡Œæ¡†æ¶ï¼š
- æ¨¡æ¿æ–¹æ³•æ¨¡å¼
- Context è‡ªåŠ¨ç®¡ç†
- é…ç½®é©±åŠ¨
- åˆ†åŒºæ¸…ç†
- å¼‚å¸¸å¤„ç†
"""
import os
import yaml
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any

from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults

from plugins.domian.context import GovernanceContext
from plugins.infra.config import Config
from plugins.infra.io_strategy import IOStrategy


class BaseGovernanceOperator(BaseOperator, ABC):
    """
    æ²»ç†ç®—å­åŸºç±»
    
    è®¾è®¡æ¨¡å¼ï¼šæ¨¡æ¿æ–¹æ³• (Template Method)
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. pre_execute()    - åˆå§‹åŒ–ï¼ˆæ¢å¤ Contextã€æ³¨å…¥ IO ç­–ç•¥ï¼‰
    2. execute_logic()  - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼ˆå­ç±»å®ç°ï¼Œin-place ä¿®æ”¹ ctxï¼‰
    3. post_execute()   - æ”¶å°¾ï¼ˆè®°å½•æ—¥å¿—ã€æ‰“å°æ‘˜è¦ï¼‰
    4. return           - è¿”å›åºåˆ—åŒ–çš„ Contextï¼ˆAirflow è‡ªåŠ¨å­˜ä¸º return_valueï¼‰
    
    å­ç±»èŒè´£ï¼š
    - å®ç° execute_logic(ctx, context) æ–¹æ³•
    - æ ¹æ®éœ€è¦æ˜¾å¼è°ƒç”¨ _clean_partition() æ¸…ç†åˆ†åŒº
    - å¦‚éœ€æ”¯æŒå¤šä¸Šæ¸¸ï¼Œé‡å†™ _restore_context() æ–¹æ³•
    """
    
    # æ¨¡æ¿å­—æ®µï¼ˆAirflow ä¼šè‡ªåŠ¨æ¸²æŸ“ Jinja æ¨¡æ¿ï¼‰
    template_fields = ('config_path',)
    
    @apply_defaults
    def __init__(
        self,
        config_path: Optional[str] = None,      # YAML é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äº plugins/ï¼‰
        config_dict: Optional[Dict] = None,     # æˆ–ç›´æ¥ä¼ å…¥é…ç½®å­—å…¸
        upstream_task_id: Optional[str] = None, # ä¸Šæ¸¸ Task IDï¼ˆç”¨äºæ¢å¤ Contextï¼‰
        **kwargs
    ):
        """
        åˆå§‹åŒ–ç®—å­
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äº $AIRFLOW_HOME/plugins/ï¼‰
            config_dict: é…ç½®å­—å…¸ï¼ˆä¼˜å…ˆçº§é«˜äº config_pathï¼‰
            upstream_task_id: ä¸Šæ¸¸ Task IDï¼ˆç”¨äºä» XCom æ¢å¤ Contextï¼‰
            **kwargs: BaseOperator çš„å…¶ä»–å‚æ•°
        """
        super().__init__(**kwargs)
        self.config_path = config_path
        self.config_dict = config_dict
        self.upstream_task_id = upstream_task_id
        
        # åŠ è½½é…ç½®ï¼ˆåœ¨ DAG è§£ææ—¶æ‰§è¡Œï¼‰
        self._config = self._load_config()
    
    # ========================================================================
    # æ¨¡æ¿æ–¹æ³•ï¼ˆTemplate Methodï¼‰
    # ========================================================================
    
    def execute(self, context: Dict[str, Any]) -> str:
        """
        ç»Ÿä¸€æ‰§è¡Œæµç¨‹ï¼ˆAirflow è°ƒç”¨çš„å…¥å£ï¼‰
        
        Args:
            context: Airflow context å­—å…¸
        
        Returns:
            åºåˆ—åŒ–çš„ Context JSON å­—ç¬¦ä¸²ï¼ˆè‡ªåŠ¨å­˜ä¸º return_valueï¼‰
        
        Raises:
            Exception: æ‰§è¡Œå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            # Step 1: åˆå§‹åŒ–é˜¶æ®µ
            ctx = self.pre_execute(context)
            
            # Step 2: æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼ˆå­ç±»å®ç°ï¼Œin-place ä¿®æ”¹ ctxï¼‰
            self.execute_logic(ctx, context)
            
            # Step 3: æ”¶å°¾é˜¶æ®µ
            self.post_execute(ctx, context)
            
            # Step 4: è¿”å›åºåˆ—åŒ–çš„ Contextï¼ˆAirflow æ ‡å‡†ï¼‰
            return ctx.to_json()
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†
            self.handle_error(e, context)
            raise
    
    # ========================================================================
    # æŠ½è±¡æ–¹æ³•ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰
    # ========================================================================
    
    @abstractmethod
    def execute_logic(self, ctx: GovernanceContext, context: Dict[str, Any]) -> None:
        """
        æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼ˆå­ç±»å®ç°ï¼‰
        
        Args:
            ctx: æ²»ç†ä¸Šä¸‹æ–‡ï¼ˆin-place ä¿®æ”¹ï¼‰
            context: Airflow context å­—å…¸
        
        Returns:
            Noneï¼ˆç›´æ¥ä¿®æ”¹ ctxï¼Œä¸è¿”å›ï¼‰
        
        ç¤ºä¾‹:
            def execute_logic(self, ctx, context):
                # 1. è¯»å–é…ç½®
                config = self._config
                
                # 2. å¤„ç†æ•°æ®
                df = self._process_data()
                
                # 3. å†™å…¥ Contextï¼ˆin-placeï¼‰
                ctx.put_dataframe('result', df, stage='ENTITY')
        """
        pass
    
    # ========================================================================
    # é’©å­æ–¹æ³•ï¼ˆHook Methodsï¼‰
    # ========================================================================
    
    def pre_execute(self, context: Dict[str, Any]) -> GovernanceContext:
        """
        åˆå§‹åŒ–é˜¶æ®µ
        
        èŒè´£ï¼š
        1. ä»ä¸Šæ¸¸ XCom æ¢å¤ Contextï¼ˆæˆ–åˆ›å»ºæ–° Contextï¼‰
        2. æ³¨å…¥ IO ç­–ç•¥ï¼ˆéªŒè¯é…ç½®ä¸€è‡´æ€§ï¼‰
        3. è®°å½•å¯åŠ¨æ—¥å¿—
        
        Args:
            context: Airflow context å­—å…¸
        
        Returns:
            GovernanceContext å¯¹è±¡
        """
        # 1. æ¢å¤ Context
        ctx = self._restore_context(context)
        
        # 2. æ³¨å…¥ IO ç­–ç•¥
        self._inject_io_strategy(ctx)
        
        # 3. è®°å½•æ—¥å¿—
        ctx.log(f"Task [{self.task_id}] started")
        self.log.info(f"Initialized context: batch_id={ctx.batch_id}, run_date={ctx.run_date}")
        
        return ctx
    
    def post_execute(self, ctx: GovernanceContext, context: Dict[str, Any]) -> None:
        """
        æ”¶å°¾é˜¶æ®µ
        
        èŒè´£ï¼š
        1. è®°å½•å®Œæˆæ—¥å¿—
        2. æ‰“å° Context æ‘˜è¦
        3. é¢„ç•™ï¼šå‘é€é€šçŸ¥ç­‰
        
        Args:
            ctx: æ²»ç†ä¸Šä¸‹æ–‡
            context: Airflow context å­—å…¸
        """
        # 1. è®°å½•æ—¥å¿—
        ctx.log(f"Task [{self.task_id}] completed")
        
        # 2. æ‰“å°æ‘˜è¦
        summary = ctx.summary()
        self.log.info(f"Context Summary: {summary}")
        
        # 3. æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        if ctx.alerts:
            self.log.info(f"Generated {len(ctx.alerts)} alerts")
        if ctx.assets:
            self.log.info(f"Generated {len(ctx.assets)} assets")
        if ctx.rule_outputs:
            self.log.info(f"Executed {len(ctx.rule_outputs)} rules")
    
    # ========================================================================
    # Context ç®¡ç†
    # ========================================================================
    
    def _restore_context(self, context: Dict[str, Any]) -> GovernanceContext:
        """
        æ¢å¤ Contextï¼ˆå•ä¸Šæ¸¸åœºæ™¯ï¼‰
        
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä»ä¸Šæ¸¸ Task çš„ return_value æ¢å¤
        2. å¦‚æœæ²¡æœ‰ä¸Šæ¸¸ï¼Œåˆ›å»ºæ–° Contextï¼ˆå…œåº•é€»è¾‘ï¼‰
        
        å­ç±»å¯é‡å†™æ­¤æ–¹æ³•ä»¥æ”¯æŒå¤šä¸Šæ¸¸åœºæ™¯ï¼ˆå¦‚ Aggregatorï¼‰
        
        Args:
            context: Airflow context å­—å…¸
        
        Returns:
            GovernanceContext å¯¹è±¡
        """
        ti = context['task_instance']
        
        # ä»ä¸Šæ¸¸æ¢å¤
        if self.upstream_task_id:
            try:
                # æ‹‰å–ä¸Šæ¸¸çš„ return_valueï¼ˆAirflow æ ‡å‡†ï¼‰
                ctx_json = ti.xcom_pull(task_ids=self.upstream_task_id)
                
                if ctx_json:
                    ctx = GovernanceContext.from_json(ctx_json)
                    self.log.info(f"âœ… Restored context from upstream: {self.upstream_task_id}")
                    return ctx
                else:
                    self.log.warning(f"âš ï¸ Upstream task [{self.upstream_task_id}] returned None")
            except Exception as e:
                self.log.warning(f"âš ï¸ Failed to restore context from upstream: {e}")
        
        # å…œåº•ï¼šåˆ›å»ºæ–° Context
        self.log.info("ğŸ“¦ Creating new context (no valid upstream found)")
        return self._create_new_context(context)
    
    def _create_new_context(self, context: Dict[str, Any]) -> GovernanceContext:
        """
        åˆ›å»ºæ–° Contextï¼ˆå…œåº•é€»è¾‘ï¼‰
        
        ç”Ÿæˆè§„åˆ™ï¼š
        1. batch_id: ä¼˜å…ˆè¯» confï¼Œå¦åˆ™ç”¨ "BATCH_{ts_nodash}"
        2. run_date: ä¼˜å…ˆè¯» confï¼Œå¦åˆ™ç”¨ ds (YYYY-MM-DD)
        3. storage_type: ä»å…¨å±€é…ç½®è¯»å–
        
        Args:
            context: Airflow context å­—å…¸
        
        Returns:
            æ–°çš„ GovernanceContext å¯¹è±¡
        """
        dag_run = context['dag_run']
        
        # ä¼˜å…ˆè¯»å– confï¼ˆæ‰‹åŠ¨è§¦å‘æ—¶ä¼šæœ‰ï¼‰
        batch_id = None
        run_date = None
        
        if dag_run.conf:
            batch_id = dag_run.conf.get('batch_id')
            run_date = dag_run.conf.get('run_date')
        
        # å…œåº•ï¼šè‡ªåŠ¨ç”Ÿæˆ
        if not batch_id:
            batch_id = f"BATCH_{context['ts_nodash']}"
        if not run_date:
            run_date = context['ds']
        
        self.log.info(f"Generated context: batch_id={batch_id}, run_date={run_date}")
        
        return GovernanceContext(
            batch_id=batch_id,
            run_date=run_date,
            storage_type=Config.get_storage_type()
        )
    
    def _inject_io_strategy(self, ctx: GovernanceContext) -> None:
        """
        æ³¨å…¥ IO ç­–ç•¥ï¼ˆéªŒè¯é…ç½®ä¸€è‡´æ€§ï¼‰
        
        è¯´æ˜ï¼š
        Context åºåˆ—åŒ–æ—¶ä¸åŒ…å« IO å®ä¾‹ï¼ˆä¸å¯åºåˆ—åŒ–ï¼‰ï¼Œ
        ä½† Context çš„æ–¹æ³•å†…éƒ¨ä¼šå»¶è¿Ÿå¯¼å…¥ IOStrategyã€‚
        è¿™é‡Œä¸»è¦æ˜¯éªŒè¯ storage_type çš„ä¸€è‡´æ€§ã€‚
        
        Args:
            ctx: æ²»ç†ä¸Šä¸‹æ–‡
        """
        global_storage_type = Config.get_storage_type()
        
        # éªŒè¯ä¸€è‡´æ€§
        if ctx.storage_type != global_storage_type:
            self.log.warning(
                f"âš ï¸ Context storage_type ({ctx.storage_type}) differs from "
                f"global config ({global_storage_type}). Using global config."
            )
            # å¼ºåˆ¶ä½¿ç”¨å…¨å±€é…ç½®
            ctx.storage_type = global_storage_type
        
        self.log.info(f"IO Strategy: {ctx.storage_type}")
    
    # ========================================================================
    # åˆ†åŒºæ¸…ç†
    # ========================================================================
    
    def _clean_partition(self, ctx: GovernanceContext, stage: str, key: str) -> None:
        """
        æ¸…ç†åˆ†åŒºç›®å½•ï¼ˆå­ç±»æ˜¾å¼è°ƒç”¨ï¼‰
        
        ä½¿ç”¨åœºæ™¯ï¼š
        - Loader: åœ¨å†™å…¥æ¯ä¸ª output_key å‰è°ƒç”¨
        - Adapter: åœ¨å†™å…¥ output_key å‰è°ƒç”¨
        - Rule: åœ¨å†™å…¥ç»“æœå‰è°ƒç”¨
        
        Args:
            ctx: æ²»ç†ä¸Šä¸‹æ–‡
            stage: æ•°æ®é˜¶æ®µ ("RAW" / "ENTITY" / "RESULT")
            key: æ•°æ®é”®å
        
        ç¤ºä¾‹:
            self._clean_partition(ctx, stage="RAW", key="mysql_summary")
        """
        # æ„å»ºåˆ†åŒºè·¯å¾„
        uri = ctx._build_partition_path(stage, key)
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
        if IOStrategy.exists(uri, ctx.storage_type):
            self.log.info(f"ğŸ§¹ Cleaning partition: {uri}")
            IOStrategy.clean_directory(uri, ctx.storage_type)
        else:
            self.log.info(f"â­ï¸ Partition does not exist, skip cleaning: {uri}")
    
    def _clean_multiple_partitions(self, ctx: GovernanceContext, stage: str, keys: list) -> None:
        """
        æ‰¹é‡æ¸…ç†å¤šä¸ªåˆ†åŒºï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            ctx: æ²»ç†ä¸Šä¸‹æ–‡
            stage: æ•°æ®é˜¶æ®µ
            keys: é”®ååˆ—è¡¨
        """
        for key in keys:
            self._clean_partition(ctx, stage, key)
    
    # ========================================================================
    # é…ç½®ç®¡ç†
    # ========================================================================
    
    def _load_config(self) -> Dict[str, Any]:
        """
        åŠ è½½é…ç½®ï¼ˆä»å­—å…¸æˆ– YAML æ–‡ä»¶ï¼‰
        
        ä¼˜å…ˆçº§ï¼š
        1. config_dictï¼ˆç›´æ¥ä¼ å…¥ï¼‰
        2. config_pathï¼ˆYAML æ–‡ä»¶ï¼‰
        3. ç©ºå­—å…¸ï¼ˆæ— é…ç½®ï¼‰
        
        Returns:
            é…ç½®å­—å…¸
        """
        # ä¼˜å…ˆä½¿ç”¨ config_dict
        if self.config_dict:
            self.log.info("Loaded config from dict")
            return self.config_dict
        
        # ä»æ–‡ä»¶åŠ è½½
        if self.config_path:
            try:
                # æ„å»ºå®Œæ•´è·¯å¾„ï¼ˆç›¸å¯¹äº $AIRFLOW_HOME/plugins/ï¼‰
                airflow_home = os.getenv('AIRFLOW_HOME', '/opt/airflow')
                full_path = os.path.join(airflow_home, 'plugins', self.config_path)
                
                # è¯»å– YAML
                with open(full_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                
                self.log.info(f"âœ… Loaded config from: {full_path}")
                return config if config else {}
                
            except FileNotFoundError:
                self.log.error(f"âŒ Config file not found: {full_path}")
                raise
            except yaml.YAMLError as e:
                self.log.error(f"âŒ Invalid YAML syntax: {e}")
                raise
        
        # æ— é…ç½®
        self.log.info("No config provided, using empty dict")
        return {}
    
    def _validate_config(self, required_keys: list) -> None:
        """
        éªŒè¯é…ç½®å®Œæ•´æ€§ï¼ˆå¯é€‰ï¼Œå­ç±»è°ƒç”¨ï¼‰
        
        Args:
            required_keys: å¿…éœ€çš„é…ç½®é”®åˆ—è¡¨
        
        Raises:
            ValueError: å¦‚æœç¼ºå°‘å¿…éœ€çš„é…ç½®
        """
        missing = [key for key in required_keys if key not in self._config]
        
        if missing:
            raise ValueError(f"Missing required config keys: {', '.join(missing)}")
    
    # ========================================================================
    # å¼‚å¸¸å¤„ç†
    # ========================================================================
    
    def handle_error(self, error: Exception, context: Dict[str, Any]) -> None:
        """
        å¼‚å¸¸å¤„ç†
        
        èŒè´£ï¼š
        1. è®°å½•è¯¦ç»†é”™è¯¯æ—¥å¿—
        2. é¢„ç•™ï¼šå‘é€å‘Šè­¦é€šçŸ¥
        3. é¢„ç•™ï¼šè®°å½•åˆ°æ•°æ®åº“
        
        Args:
            error: å¼‚å¸¸å¯¹è±¡
            context: Airflow context å­—å…¸
        """
        self.log.error(f"âŒ Task [{self.task_id}] failed: {error}")
        self.log.error(f"Error type: {type(error).__name__}")
        
        # æ‰“å°å †æ ˆï¼ˆè°ƒè¯•ç”¨ï¼‰
        import traceback
        self.log.error(f"Stacktrace:\n{traceback.format_exc()}")
        
        # é¢„ç•™ï¼šå‘é€å‘Šè­¦
        # self._send_alert(error, context)
    
    # ========================================================================
    # å·¥å…·æ–¹æ³•
    # ========================================================================
    
    def _get_airflow_context_summary(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        è·å– Airflow Context æ‘˜è¦ï¼ˆè°ƒè¯•ç”¨ï¼‰
        
        Args:
            context: Airflow context å­—å…¸
        
        Returns:
            æ‘˜è¦å­—å…¸
        """
        return {
            'dag_id': context['dag'].dag_id,
            'task_id': self.task_id,
            'execution_date': str(context['execution_date']),
            'run_id': context['dag_run'].run_id,
            'ds': context['ds'],
            'ts': context['ts'],
        }


# ============================================================================
# ä¾¿æ·å‡½æ•°ï¼ˆä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨ï¼‰
# ============================================================================

def get_upstream_context(ti, task_id: str) -> Optional[GovernanceContext]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä»ä¸Šæ¸¸ Task è·å– Context
    
    Args:
        ti: TaskInstance å¯¹è±¡
        task_id: ä¸Šæ¸¸ Task ID
    
    Returns:
        GovernanceContext å¯¹è±¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    """
    ctx_json = ti.xcom_pull(task_ids=task_id)
    if ctx_json:
        return GovernanceContext.from_json(ctx_json)
    return None


def get_multiple_upstream_contexts(ti, task_ids: list) -> list:
    """
    ä¾¿æ·å‡½æ•°ï¼šä»å¤šä¸ªä¸Šæ¸¸ Task è·å– Context
    
    Args:
        ti: TaskInstance å¯¹è±¡
        task_ids: ä¸Šæ¸¸ Task ID åˆ—è¡¨
    
    Returns:
        Context å¯¹è±¡åˆ—è¡¨
    """
    contexts = []
    for task_id in task_ids:
        ctx = get_upstream_context(ti, task_id)
        if ctx:
            contexts.append(ctx)
    return contexts
