"""
operators/loader.py
é€šç”¨æ•°æ®åŠ è½½å™¨

èŒè´£ï¼š
- ä»å¤šä¸ªæ•°æ®æºï¼ˆMySQL/InfluxDBï¼‰æŠ½å–æ•°æ®
- æ¸²æŸ“ SQL æ¨¡æ¿ï¼ˆæ”¯æŒ Jinjaï¼‰
- å†™å…¥ Raw Parquet
- æ”¯æŒå¤šè¡¨æŠ½å–
"""
import polars as pl
from typing import Dict, Any
from jinja2 import Template

from airflow.providers.mysql.hooks.mysql import MySqlHook
from plugins.infra.operators import BaseGovernanceOperator
from plugins.domian.context import GovernanceContext


class UniversalLoaderOperator(BaseGovernanceOperator):
    """
    é€šç”¨æ•°æ®åŠ è½½å™¨
    
    é…ç½®ç¤ºä¾‹ (configs/sources/datalog_mysql.yaml):
    ```yaml
    source_meta:
      id: "mysql_datalog_raw"
      type: "mysql"
      connection_id: "datalog_mysql_conn"
    
    extractions:
      - id: "summary"
        output_key: "raw_summary"
        sql: "SELECT * FROM cycle_section_summary WHERE shift_date = '{{ ds }}'"
        alt_key: "Summary"
      
      - id: "subtarget"
        output_key: "raw_subtarget"
        sql: "SELECT * FROM subtarget_vehicle_cycle WHERE SHIFT_DATE = '{{ ds }}'"
        alt_key: "Subtarget"
    ```
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    loader = UniversalLoaderOperator(
        task_id='load_raw_data',
        config_path='configs/sources/datalog_mysql.yaml',
        dag=dag
    )
    ```
    """
    
    def execute_logic(self, ctx: GovernanceContext, context: Dict[str, Any]) -> None:
        """
        æŠ½å–é€»è¾‘
        
        æµç¨‹ï¼š
        1. éå†é…ç½®ä¸­çš„æ‰€æœ‰æŠ½å–ä»»åŠ¡
        2. å¯¹æ¯ä¸ªä»»åŠ¡ï¼šæ¸…ç† â†’ æŠ½å– â†’ å†™å…¥
        3. è®°å½•æŠ½å–ç»Ÿè®¡
        """
        # éªŒè¯é…ç½®
        self._validate_config(['source_meta', 'extractions'])
        
        source_meta = self._config['source_meta']
        extractions = self._config.get('extractions', [])
        
        if not extractions:
            self.log.warning("No extractions configured, skipping...")
            return
        
        self.log.info(f"Starting extractions: {len(extractions)} tasks")
        
        # æ‰§è¡Œæ¯ä¸ªæŠ½å–ä»»åŠ¡
        for task in extractions:
            self._execute_extraction_task(task, source_meta, ctx, context)
        
        self.log.info(f"âœ… Completed {len(extractions)} extractions")
    
    def _execute_extraction_task(
        self, 
        task: Dict[str, Any], 
        source_meta: Dict[str, Any],
        ctx: GovernanceContext, 
        context: Dict[str, Any]
    ) -> None:
        """
        æ‰§è¡Œå•ä¸ªæŠ½å–ä»»åŠ¡
        
        Args:
            task: æŠ½å–ä»»åŠ¡é…ç½®
            source_meta: æ•°æ®æºå…ƒä¿¡æ¯
            ctx: æ²»ç†ä¸Šä¸‹æ–‡
            context: Airflow context
        """
        task_id = task.get('id', 'unknown')
        output_key = task['output_key']
        
        self.log.info(f"ğŸ“¥ Extracting [{task_id}] -> {output_key}")
        
        try:
            # 1. æ¸…ç†æ—§æ•°æ®
            self._clean_partition(ctx, stage="RAW", key=output_key)
            
            # 2. æŠ½å–æ•°æ®
            df = self._extract_from_source(task, source_meta, context)
            
            # 3. æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
            if df.height == 0:
                self.log.warning(f"âš ï¸ [{task_id}] extracted 0 rows")
            
            # 4. å†™å…¥ Context
            ctx.put_dataframe(
                key=output_key,
                df=df,
                stage="RAW",
                alt_key=task.get('alt_key')
            )
            
            self.log.info(f"âœ… [{task_id}] extracted {df.height} rows")
            
        except Exception as e:
            self.log.error(f"âŒ [{task_id}] extraction failed: {e}")
            raise
    
    def _extract_from_source(
        self, 
        task: Dict[str, Any], 
        source_meta: Dict[str, Any],
        context: Dict[str, Any]
    ) -> pl.DataFrame:
        """
        ä»æ•°æ®æºæŠ½å–æ•°æ®
        
        Args:
            task: æŠ½å–ä»»åŠ¡é…ç½®
            source_meta: æ•°æ®æºå…ƒä¿¡æ¯
            context: Airflow context
        
        Returns:
            Polars DataFrame
        """
        # 1. æ¸²æŸ“ SQLï¼ˆå¤„ç† Jinja æ¨¡æ¿ï¼‰
        # æ”¯æŒ 'query' å’Œ 'sql' å­—æ®µï¼ˆå…¼å®¹ä¸åŒé…ç½®æ ¼å¼ï¼‰
        sql_template = task.get('query') or task.get('sql')
        if not sql_template:
            raise ValueError(f"Missing 'query' or 'sql' field in extraction config: {task.get('id')}")
        
        sql = self._render_sql(sql_template, context)
        
        self.log.info(f"Rendered SQL: {sql[:200]}...")  # æ‰“å°å‰200å­—ç¬¦
        
        # 2. æ ¹æ®æ•°æ®æºç±»å‹æŠ½å–
        source_type = task.get('source_type', 'mysql')
        
        if source_type == 'mysql':
            return self._extract_from_mysql(sql, task)
        elif source_type == 'postgresql':
            # é¢„ç•™ï¼šPostgreSQL æŠ½å–é€»è¾‘
            raise NotImplementedError("PostgreSQL support not implemented yet")
        elif source_type == 'influxdb':
            # é¢„ç•™ï¼šInfluxDB æŠ½å–é€»è¾‘
            raise NotImplementedError("InfluxDB support not implemented yet")
        else:
            raise ValueError(f"Unsupported source type: {source_type}")
    
    def _extract_from_mysql(
        self, 
        sql: str, 
        task: Dict[str, Any]
    ) -> pl.DataFrame:
        """
        ä» MySQL æŠ½å–æ•°æ®ï¼ˆä½¿ç”¨ Airflow Hook + Pandas ä¸­è½¬ï¼‰
        
        ç­–ç•¥ï¼šHook â†’ Pandas â†’ Polars
        ä¼˜ç‚¹ï¼šæ— å¤–éƒ¨ä¾èµ–ï¼ˆconnectorxï¼‰ï¼Œç¨³å®šå¯é 
        
        Args:
            sql: æ¸²æŸ“åçš„ SQL
            task: æŠ½å–ä»»åŠ¡é…ç½®ï¼ˆåŒ…å« conn_idï¼‰
        
        Returns:
            Polars DataFrame
        """
        # æ”¯æŒ 'conn_id' å’Œ 'connection_id' å­—æ®µï¼ˆå…¼å®¹ä¸åŒé…ç½®æ ¼å¼ï¼‰
        connection_id = task.get('conn_id') or task.get('connection_id')
        if not connection_id:
            raise ValueError(f"Missing 'conn_id' or 'connection_id' in extraction task: {task.get('id')}")
        
        try:
            # 1. ä½¿ç”¨ MySqlHook è·å– Pandas DataFrame
            hook = MySqlHook(mysql_conn_id=connection_id)
            self.log.info(f"ğŸ”Œ Connecting to MySQL via Hook: {connection_id}")
            
            pandas_df = hook.get_pandas_df(sql=sql)
            self.log.info(f"âœ… Fetched {len(pandas_df)} rows from MySQL")
            
            # 2. å¤„ç†ç©ºç»“æœé›†
            if pandas_df.empty:
                self.log.warning("âš ï¸ Query returned empty result")
                return pl.from_pandas(pandas_df)
            
            # 3. è½¬æ¢ä¸º Polars DataFrame
            polars_df = pl.from_pandas(pandas_df)
            self.log.info(
                f"âœ… Converted to Polars: {polars_df.height} rows Ã— "
                f"{polars_df.width} columns"
            )
            
            return polars_df
            
        except Exception as e:
            self.log.error(f"âŒ MySQL extraction failed: {str(e)}")
            self.log.error(f"SQL preview: {sql[:500]}...")
            raise
    
    def _render_sql(self, sql_template: str, context: Dict[str, Any]) -> str:
        """
        æ¸²æŸ“ SQL æ¨¡æ¿ï¼ˆæ”¯æŒ Jinjaï¼‰
        
        Args:
            sql_template: SQL æ¨¡æ¿å­—ç¬¦ä¸²
            context: Airflow contextï¼ˆåŒ…å« ds, ts ç­‰å®ï¼‰
        
        Returns:
            æ¸²æŸ“åçš„ SQL å­—ç¬¦ä¸²
        
        ç¤ºä¾‹:
            æ¨¡æ¿: "SELECT * FROM table WHERE date = '{{ ds }}'"
            æ¸²æŸ“: "SELECT * FROM table WHERE date = '2026-01-26'"
        """
        template = Template(sql_template)
        
        # æå–å¸¸ç”¨å®
        template_vars = {
            'ds': context.get('ds'),                    # 2026-01-26
            'ds_nodash': context.get('ds_nodash'),      # 20260126
            'ts': context.get('ts'),                    # 2026-01-26T12:00:00+00:00
            'ts_nodash': context.get('ts_nodash'),      # 20260126T120000
            'execution_date': context.get('execution_date'),
            'prev_ds': context.get('prev_ds'),
            'next_ds': context.get('next_ds'),
            'yesterday_ds': context.get('yesterday_ds'),
            'tomorrow_ds': context.get('tomorrow_ds'),
        }
        
        # æ¸²æŸ“
        rendered_sql = template.render(**template_vars)
        
        return rendered_sql
