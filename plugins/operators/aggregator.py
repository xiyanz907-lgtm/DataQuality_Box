"""
operators/aggregator.py
ä¸Šä¸‹æ–‡èšåˆå™¨ï¼ˆReduce é˜¶æ®µï¼‰

èŒè´£ï¼š
- åˆå¹¶å¤šä¸ª Rule Task çš„ Context
- è¯»å–æ‰€æœ‰ Rule Result Parquet
- å¤„ç† P0/P1/P2 é€»è¾‘å’Œç™½åå•è±å…
- ç”Ÿæˆæœ€ç»ˆçš„ Alerts å’Œ Assets
"""
import polars as pl
from typing import Dict, Any, List

from plugins.infra.operators import BaseGovernanceOperator, get_multiple_upstream_contexts
from plugins.domian.context import GovernanceContext


class ContextAggregatorOperator(BaseGovernanceOperator):
    """
    ä¸Šä¸‹æ–‡èšåˆå™¨
    
    èŒè´£ï¼š
    1. åˆå¹¶å¤šä¸ª Rule Task çš„ Contextï¼ˆrule_outputsï¼‰
    2. è¯»å–æ‰€æœ‰å‘½ä¸­çš„ Result Parquet
    3. æ ¹æ® _severity åˆ—å¤„ç†ä¸åŒç­‰çº§çš„è§„åˆ™
    4. P0 â†’ ç›´æ¥ç”Ÿæˆ Alert
    5. P1 â†’ ç”Ÿæˆ Assetï¼ˆæ ‡è®°ç™½åå•ï¼‰
    6. P2 â†’ æ£€æŸ¥ç™½åå•åç”Ÿæˆ Alert
    
    é…ç½®ç¤ºä¾‹:
    ```yaml
    # å¯é€‰é…ç½®ï¼ˆå¦‚ Asset æ‰“åŒ…è·¯å¾„æ¨¡æ¿ï¼‰
    asset_path_template: "corner_case/{batch_id}/{cycle_id}/"
    ```
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    aggregator = ContextAggregatorOperator(
        task_id='aggregate_context',
        rule_task_ids=['rule_p0', 'rule_p1', 'rule_p2'],  # æ‰€æœ‰ä¸Šæ¸¸ Rule Task
        dag=dag
    )
    ```
    """
    
    def __init__(self, upstream_task_ids: List[str] = None, rule_task_ids: List[str] = None, **kwargs):
        """
        åˆå§‹åŒ–
        
        Args:
            upstream_task_ids: ä¸Šæ¸¸ Task çš„ ID åˆ—è¡¨ï¼ˆæ¨èä½¿ç”¨ï¼‰
            rule_task_ids: ä¸Šæ¸¸ Rule Task çš„ ID åˆ—è¡¨ï¼ˆå…¼å®¹æ—§å‚æ•°åï¼‰
        """
        # å…¼å®¹ä¸¤ç§å‚æ•°å
        if upstream_task_ids is not None:
            self.rule_task_ids = upstream_task_ids
        elif rule_task_ids is not None:
            self.rule_task_ids = rule_task_ids
        else:
            raise ValueError("Must provide either 'upstream_task_ids' or 'rule_task_ids'")
        
        super().__init__(**kwargs)
    
    def _restore_context(self, context: Dict[str, Any]) -> GovernanceContext:
        """
        é‡å†™ï¼šæ”¯æŒå¤šä¸Šæ¸¸åˆå¹¶
        
        Args:
            context: Airflow context å­—å…¸
        
        Returns:
            åˆå¹¶åçš„ GovernanceContext
        """
        ti = context['task_instance']
        
        # æ‹‰å–æ‰€æœ‰ä¸Šæ¸¸ Context
        contexts = get_multiple_upstream_contexts(ti, self.rule_task_ids)
        
        if not contexts:
            raise ValueError("No upstream contexts found! Check rule_task_ids.")
        
        self.log.info(f"Merging {len(contexts)} upstream contexts")
        
        # åˆå¹¶ Contextï¼ˆå–ç¬¬ä¸€ä¸ªä½œä¸ºåŸºç¡€ï¼‰
        merged_ctx = contexts[0]
        
        # åˆå¹¶å…¶ä»– Context çš„ rule_outputsã€data_registry å’Œ audit_logs
        for ctx in contexts[1:]:
            merged_ctx.rule_outputs.update(ctx.rule_outputs)
            merged_ctx.data_registry.update(ctx.data_registry)  # â­ ä¿®å¤ï¼šåˆå¹¶ data_registry
            merged_ctx.audit_logs.extend(ctx.audit_logs)
        
        self.log.info(f"âœ… Merged context: {len(merged_ctx.rule_outputs)} rule outputs, {len(merged_ctx.data_registry)} data refs")
        
        return merged_ctx
    
    def execute_logic(self, ctx: GovernanceContext, context: Dict[str, Any]) -> None:
        """
        èšåˆé€»è¾‘
        
        æµç¨‹ï¼š
        1. å¤„ç† P0 å‘Šè­¦ï¼ˆç›´æ¥ç”Ÿæˆï¼‰
        2. å¤„ç† P1 èµ„äº§ï¼ˆç”Ÿæˆ Assetsï¼‰
        3. å¤„ç† P2 è¿è§„ï¼ˆç™½åå•è±å…ï¼‰
        4. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        """
        self.log.info("ğŸ”„ Starting context aggregation...")
        
        # ç»Ÿè®¡è§„åˆ™æ‰§è¡Œç»“æœ
        total_rules = len(ctx.rule_outputs)
        success_rules = sum(1 for r in ctx.rule_outputs.values() if r.status == "SUCCESS")
        failed_rules = sum(1 for r in ctx.rule_outputs.values() if r.status == "FAILED")
        total_hits = sum(r.hit_count for r in ctx.rule_outputs.values())
        
        self.log.info(
            f"Rule execution summary: "
            f"{total_rules} total, {success_rules} success, {failed_rules} failed, {total_hits} hits"
        )
        
        # 1. å¤„ç† P0 å‘Šè­¦
        self._process_p0_alerts(ctx)
        
        # 2. å¤„ç† P1 èµ„äº§
        self._process_p1_assets(ctx)
        
        # 3. å¤„ç† P2 è¿è§„
        self._process_p2_violations(ctx)
        
        # 4. æ‰“å°æœ€ç»ˆç»Ÿè®¡
        self.log.info(f"âœ… Aggregation completed: {len(ctx.alerts)} alerts, {len(ctx.assets)} assets")
        
        # 5. æ¨é€ GovernanceContext åˆ° XComï¼ˆä¾› save_assets_to_queue ä½¿ç”¨ï¼‰
        ctx_json = ctx.to_json()
        context['ti'].xcom_push(key='governance_context', value=ctx_json)
        self.log.info(f"ğŸ“¤ Pushed GovernanceContext to XCom (key='governance_context')")
    
    def _process_p0_alerts(self, ctx: GovernanceContext) -> None:
        """
        å¤„ç† P0 å‘Šè­¦ï¼ˆç›´æ¥ç”Ÿæˆï¼‰
        
        Args:
            ctx: æ²»ç†ä¸Šä¸‹æ–‡
        """
        self.log.info("ğŸ“‹ Processing P0 alerts...")
        
        p0_count = 0
        
        for rule_id, rule_output in ctx.rule_outputs.items():
            if rule_output.hit_count > 0:
                try:
                    # è¯»å–å‘½ä¸­æ•°æ®
                    hit_df = ctx.get_dataframe(key=f"{rule_id}_hits", use_alt_key=False)
                    
                    # âœ… ä½¿ç”¨ _severity åˆ—è¿‡æ»¤ï¼ˆä¸è§£æå­—ç¬¦ä¸²ï¼‰
                    p0_df = hit_df.filter(pl.col('_severity') == 'P0')
                    
                    if p0_df.height > 0:
                        # æå– cycle_id åˆ—è¡¨
                        cycle_ids = p0_df['cycle_id'].to_list()
                        
                        # ç”Ÿæˆå‘Šè­¦
                        ctx.add_alert(
                            rule_id=rule_id,
                            severity="P0",
                            title=f"[P0] æ•°æ®è´¨é‡å¼‚å¸¸ - {rule_id}",
                            content=f"å‘ç° {len(cycle_ids)} æ¡å¼‚å¸¸æ•°æ®",
                            trigger_cycle_ids=cycle_ids
                        )
                        
                        p0_count += 1
                        self.log.info(f"  âœ… Generated P0 alert: {rule_id} ({len(cycle_ids)} hits)")
                
                except Exception as e:
                    self.log.warning(f"  âš ï¸ Failed to process rule {rule_id}: {e}")
        
        self.log.info(f"ğŸ“‹ P0 alerts processed: {p0_count} alerts generated")
    
    def _process_p1_assets(self, ctx: GovernanceContext) -> None:
        """
        å¤„ç† P1 èµ„äº§ï¼ˆç”Ÿæˆ Assetsï¼‰
        
        Args:
            ctx: æ²»ç†ä¸Šä¸‹æ–‡
        """
        self.log.info("ğŸ“¦ Processing P1 assets...")
        
        asset_count = 0
        asset_path_template = self._config.get('asset_path_template', 'corner_case/{batch_id}/{cycle_id}/')
        
        for rule_id, rule_output in ctx.rule_outputs.items():
            if rule_output.hit_count > 0:
                try:
                    # è¯»å–å‘½ä¸­æ•°æ®
                    hit_df = ctx.get_dataframe(key=f"{rule_id}_hits", use_alt_key=False)
                    
                    # âœ… ä½¿ç”¨ _severity åˆ—è¿‡æ»¤
                    p1_df = hit_df.filter(pl.col('_severity') == 'P1')
                    
                    if p1_df.height > 0:
                        # ç”Ÿæˆ Assets
                        for row in p1_df.iter_rows(named=True):
                            # æ„å»ºç›®æ ‡è·¯å¾„
                            target_path = asset_path_template.format(
                                batch_id=ctx.batch_id,
                                cycle_id=row['cycle_id']
                            )
                            
                            ctx.add_asset(
                                asset_id=row['cycle_id'],
                                asset_type="HIGH_VALUE_SCENARIO",  # å¯ä»é…ç½®è¯»å–
                                rule_id=rule_id,  # æ·»åŠ è§„åˆ™ID
                                vehicle_id=row['vehicle_id'],
                                start_ts=str(row['start_time']),
                                end_ts=str(row['end_time']),
                                tags=["VALID_TWIN_LIFT"],  # æ ‡è®°ä¸ºç™½åå•
                                target_path=target_path
                            )
                        
                        asset_count += p1_df.height
                        self.log.info(f"  âœ… Generated P1 assets: {rule_id} ({p1_df.height} assets)")
                
                except Exception as e:
                    self.log.warning(f"  âš ï¸ Failed to process rule {rule_id}: {e}")
        
        self.log.info(f"ğŸ“¦ P1 assets processed: {asset_count} assets generated")
    
    def _process_p2_violations(self, ctx: GovernanceContext) -> None:
        """
        å¤„ç† P2 è¿è§„ï¼ˆç™½åå•è±å…ï¼‰
        
        é€»è¾‘ï¼š
        1. æå– P1 èµ„äº§çš„ cycle_id ä½œä¸ºç™½åå•
        2. è¯»å– P2 è§„åˆ™çš„å‘½ä¸­æ•°æ®
        3. è¿‡æ»¤æ‰ç™½åå•ä¸­çš„ cycle_id
        4. ç”Ÿæˆå‘Šè­¦
        
        Args:
            ctx: æ²»ç†ä¸Šä¸‹æ–‡
        """
        self.log.info("âš ï¸ Processing P2 violations...")
        
        # 1. æå–ç™½åå•ï¼ˆP1 èµ„äº§çš„ cycle_idï¼‰
        exempted_ids = set(asset.asset_id for asset in ctx.assets)
        
        if exempted_ids:
            self.log.info(f"  ç™½åå•: {len(exempted_ids)} cycle_ids")
        
        p2_count = 0
        
        for rule_id, rule_output in ctx.rule_outputs.items():
            if rule_output.hit_count > 0:
                try:
                    # è¯»å–å‘½ä¸­æ•°æ®
                    hit_df = ctx.get_dataframe(key=f"{rule_id}_hits", use_alt_key=False)
                    
                    # âœ… ä½¿ç”¨ _severity åˆ—è¿‡æ»¤
                    p2_df = hit_df.filter(pl.col('_severity') == 'P2')
                    
                    if p2_df.height > 0:
                        # 2. è¿‡æ»¤ç™½åå•
                        if exempted_ids:
                            final_df = p2_df.filter(
                                ~pl.col('cycle_id').is_in(list(exempted_ids))
                            )
                        else:
                            final_df = p2_df
                        
                        # 3. ç”Ÿæˆå‘Šè­¦
                        if final_df.height > 0:
                            cycle_ids = final_df['cycle_id'].to_list()
                            
                            ctx.add_alert(
                                rule_id=rule_id,
                                severity="P2",
                                title=f"[P2] SLA è¿è§„ - {rule_id}",
                                content=f"å‘ç° {len(cycle_ids)} æ¡è¶…æ—¶æ•°æ®ï¼ˆå·²æ’é™¤ç™½åå• {len(exempted_ids)} æ¡ï¼‰",
                                trigger_cycle_ids=cycle_ids
                            )
                            
                            p2_count += 1
                            self.log.info(
                                f"  âœ… Generated P2 alert: {rule_id} "
                                f"({len(cycle_ids)} violations after exemption)"
                            )
                
                except Exception as e:
                    self.log.warning(f"  âš ï¸ Failed to process rule {rule_id}: {e}")
        
        self.log.info(f"âš ï¸ P2 violations processed: {p2_count} alerts generated")
