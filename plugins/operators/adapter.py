"""
operators/adapter.py
é¢†åŸŸé€‚é…å™¨

èŒè´£ï¼š
- å°† Raw Parquet è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„ Entity Parquet
- æ‰§è¡Œé“¾å¼ Join
- åº”ç”¨å­—æ®µæ˜ å°„å’Œè®¡ç®—
- æ”¯æŒä½ä»£ç è¡¨è¾¾å¼ï¼ˆPolars Expressionï¼‰
"""
import polars as pl
from polars import col, lit, when
from typing import Dict, Any

from plugins.infra.operators import BaseGovernanceOperator
from plugins.domian.context import GovernanceContext


class DomainAdapterOperator(BaseGovernanceOperator):
    """
    é¢†åŸŸé€‚é…å™¨
    
    é…ç½®ç¤ºä¾‹ (configs/adapters/cycle_adapter.yaml):
    ```yaml
    target_entity: "Cycle"
    output_key: "entity_cycle"
    
    input_schema:
      primary_source: "raw_summary"
      joins:
        - join_source: "raw_subtarget"
          type: "left"
          left_on: "cycle_id"
          right_on: "TRACTOR_CYCLE_ID"
          suffix: "_sub"
    
    fields:
      - target: "cycle_id"
        source_expr: "col('cycle_id')"
      
      - target: "vehicle_id"
        source_expr: "col('vehicle_id')"
      
      - target: "start_time"
        source_expr: "col('cycle_start_time').cast(pl.Datetime)"
      
      - target: "end_time"
        source_expr: "col('cycle_end_time').cast(pl.Datetime)"
      
      - target: "duration"
        source_expr: "(col('cycle_end_time') - col('cycle_start_time')).dt.total_seconds()"
      
      - target: "is_twin_lift"
        source_expr: "col('TWIN_LIFT_INDICATOR') == 1"
    ```
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    adapter = DomainAdapterOperator(
        task_id='adapt_cycle_entity',
        config_path='configs/adapters/cycle_adapter.yaml',
        upstream_task_id='load_raw_data',
        dag=dag
    )
    ```
    """
    
    def execute_logic(self, ctx: GovernanceContext, context: Dict[str, Any]) -> None:
        """
        é€‚é…é€»è¾‘
        
        æµç¨‹ï¼š
        1. åŠ è½½ä¸»è¡¨ï¼ˆprimary_sourceï¼‰
        2. æ‰§è¡Œé“¾å¼ Join
        3. åº”ç”¨å­—æ®µæ˜ å°„å’Œè®¡ç®—
        4. å†™å…¥æ ‡å‡†åŒ–å®ä½“
        """
        # éªŒè¯é…ç½®
        self._validate_config(['target_entity', 'output_key', 'input_schema', 'fields'])
        
        target_entity = self._config['target_entity']
        output_key = self._config['output_key']
        
        self.log.info(f"ğŸ”„ Adapting to entity: {target_entity}")
        
        # 1. åŠ è½½ä¸»è¡¨
        df = self._load_primary_source(ctx)
        self.log.info(f"Loaded primary source: {df.height} rows, {len(df.columns)} columns")
        
        # 2. æ‰§è¡Œé“¾å¼ Join
        df = self._apply_joins(df, ctx)
        self.log.info(f"After joins: {df.height} rows, {len(df.columns)} columns")
        
        # 3. åº”ç”¨å­—æ®µæ˜ å°„
        df_entity = self._apply_field_mapping(df)
        self.log.info(f"After mapping: {df_entity.height} rows, {len(df_entity.columns)} columns")
        
        # 4. æ¸…ç†æ—§æ•°æ®
        self._clean_partition(ctx, stage="ENTITY", key=output_key)
        
        # 5. å†™å…¥ Context
        ctx.put_dataframe(
            key=output_key,
            df=df_entity,
            stage="ENTITY",
            alt_key=self._config.get('alt_key', target_entity)
        )
        
        self.log.info(f"âœ… Entity [{target_entity}] adapted successfully")
    
    def _load_primary_source(self, ctx: GovernanceContext) -> pl.DataFrame:
        """
        åŠ è½½ä¸»è¡¨
        
        Args:
            ctx: æ²»ç†ä¸Šä¸‹æ–‡
        
        Returns:
            ä¸»è¡¨ DataFrame
        """
        primary_key = self._config['input_schema']['primary_source']
        
        # ä½¿ç”¨ alt_key æŸ¥æ‰¾ï¼ˆæ”¯æŒåˆ«åï¼‰
        df = ctx.get_dataframe(key=primary_key, use_alt_key=True)
        
        return df
    
    def _apply_joins(self, df: pl.DataFrame, ctx: GovernanceContext) -> pl.DataFrame:
        """
        åº”ç”¨é“¾å¼ Join
        
        Args:
            df: ä¸»è¡¨ DataFrame
            ctx: æ²»ç†ä¸Šä¸‹æ–‡
        
        Returns:
            Join åçš„ DataFrame
        """
        joins = self._config['input_schema'].get('joins', [])
        
        if not joins:
            self.log.info("No joins configured, skipping...")
            return df
        
        for idx, join_config in enumerate(joins):
            join_source = join_config['join_source']
            join_type = join_config['type']
            left_on = join_config['left_on']
            right_on = join_config['right_on']
            suffix = join_config.get('suffix', '_right')
            
            self.log.info(
                f"Join {idx + 1}/{len(joins)}: "
                f"{join_source} ({join_type}) on {left_on}={right_on}"
            )
            
            # åŠ è½½å³è¡¨
            df_right = ctx.get_dataframe(key=join_source, use_alt_key=True)
            
            # æ‰§è¡Œ Join
            df = df.join(
                df_right,
                left_on=left_on,
                right_on=right_on,
                how=join_type,
                suffix=suffix
            )
        
        return df
    
    def _apply_field_mapping(self, df: pl.DataFrame) -> pl.DataFrame:
        """
        åº”ç”¨å­—æ®µæ˜ å°„å’Œè®¡ç®—
        
        Args:
            df: è¾“å…¥ DataFrame
        
        Returns:
            æ˜ å°„åçš„ DataFrameï¼ˆåªåŒ…å«ç›®æ ‡å­—æ®µï¼‰
        """
        fields = self._config['fields']
        
        # æ„å»ºé€‰æ‹©è¡¨è¾¾å¼
        select_exprs = []
        
        for field_map in fields:
            target = field_map['target']
            source_expr_str = field_map['source_expr']
            
            try:
                # è§£æè¡¨è¾¾å¼
                expr = self._parse_expression(source_expr_str).alias(target)
                select_exprs.append(expr)
                
            except Exception as e:
                self.log.error(f"Failed to parse expression for field '{target}': {e}")
                self.log.error(f"Expression: {source_expr_str}")
                raise
        
        # æ‰§è¡ŒæŠ•å½±ï¼ˆåªä¿ç•™ç›®æ ‡å­—æ®µï¼‰
        df_mapped = df.select(select_exprs)
        
        return df_mapped
    
    def _parse_expression(self, expr_str: str):
        """
        å®‰å…¨è§£æ Polars è¡¨è¾¾å¼
        
        Args:
            expr_str: è¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼ˆå¦‚ "col('name').cast(pl.Utf8)"ï¼‰
        
        Returns:
            Polars Expression å¯¹è±¡
        
        å®‰å…¨æ€§ï¼š
        - é™åˆ¶ eval ä½œç”¨åŸŸï¼Œåªå…è®¸ Polars å‡½æ•°
        - ç¦æ­¢è®¿é—® __builtins__
        """
        # å®šä¹‰å®‰å…¨ä¸Šä¸‹æ–‡ï¼ˆåªå…è®¸ Polars ç›¸å…³å‡½æ•°ï¼‰
        safe_context = {
            'col': col,
            'lit': lit,
            'when': when,
            'pl': pl,
            '__builtins__': {}  # ç¦æ­¢å†…ç½®å‡½æ•°
        }
        
        # è§£æè¡¨è¾¾å¼
        try:
            expr = eval(expr_str, safe_context)
            return expr
        except Exception as e:
            raise ValueError(f"Invalid expression: {expr_str}. Error: {e}")
