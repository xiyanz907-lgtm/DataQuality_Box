#!/bin/bash
# ============================================================
# DAG B å•è¡¨æ–¹æ¡ˆæµ‹è¯•è„šæœ¬
# åŠŸèƒ½ï¼šéªŒè¯å®Œæ•´çš„èµ„äº§æ‰“åŒ…æµç¨‹
# ============================================================

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "ğŸ§ª Starting DAG B Single Table Test..."
echo "=================================================="

# ============================================================
# é…ç½®å‚æ•°ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
# ============================================================
MYSQL_CONTAINER="deploy-mysql-1"
AIRFLOW_CONTAINER="deploy-airflow-1"
MYSQL_USER="root"
MYSQL_PASS="your_password"  # è¯·ä¿®æ”¹
DATABASE="qa"                # è¯·ä¿®æ”¹

# ============================================================
# Test 1: æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„
# ============================================================
echo ""
echo "ğŸ“‹ Test 1: Checking table structure..."

echo "Checking triggered_rule_id field..."
docker exec -i $MYSQL_CONTAINER mysql -u$MYSQL_USER -p$MYSQL_PASS $DATABASE <<EOF
SELECT 
    CASE 
        WHEN COUNT(*) = 1 THEN 'âœ… triggered_rule_id field exists'
        ELSE 'âŒ triggered_rule_id field NOT FOUND'
    END AS status
FROM information_schema.columns
WHERE table_schema = '$DATABASE'
  AND table_name = 'auto_test_case_catalog'
  AND column_name = 'triggered_rule_id';
EOF

echo "Checking pack_* fields..."
docker exec -i $MYSQL_CONTAINER mysql -u$MYSQL_USER -p$MYSQL_PASS $DATABASE <<EOF
SELECT 
    column_name,
    CASE 
        WHEN column_name LIKE 'pack_%' THEN 'âœ… EXISTS'
        ELSE 'âœ… EXISTS'
    END AS status
FROM information_schema.columns
WHERE table_schema = '$DATABASE'
  AND table_name = 'auto_test_case_catalog'
  AND column_name IN ('pack_key', 'pack_url', 'pack_base_path', 'pack_poll_count', 
                      'pack_retry_count', 'pack_error_message', 'pack_started_at', 
                      'pack_completed_at', 'updated_at')
ORDER BY column_name;
EOF

# ============================================================
# Test 2: æ£€æŸ¥ Airflow DAG çŠ¶æ€
# ============================================================
echo ""
echo "ğŸ“‹ Test 2: Checking Airflow DAG status..."

# æ£€æŸ¥ DAG B
docker exec $AIRFLOW_CONTAINER airflow dags list | grep -q "asset_packing_dag" && \
    echo "âœ… DAG B (asset_packing_dag) found" || \
    echo "âŒ DAG B NOT FOUND"

# æ£€æŸ¥ Dataset
docker exec $AIRFLOW_CONTAINER airflow datasets list | grep -q "auto_test_case_catalog" && \
    echo "âœ… Dataset (auto_test_case_catalog) found" || \
    echo "âŒ Dataset NOT FOUND"

# ============================================================
# Test 3: æ¸…ç†æ—§æµ‹è¯•æ•°æ®
# ============================================================
echo ""
echo "ğŸ“‹ Test 3: Cleaning up old test data..."

docker exec -i $MYSQL_CONTAINER mysql -u$MYSQL_USER -p$MYSQL_PASS $DATABASE <<EOF
DELETE FROM auto_test_case_catalog 
WHERE cycle_id LIKE 'TEST_%' 
   OR batch_id LIKE 'TEST_%';

SELECT 'âœ… Old test data cleaned' AS status;
EOF

# ============================================================
# Test 4: æ’å…¥æµ‹è¯•æ•°æ®ï¼ˆæ¨¡æ‹Ÿ DAG A å†™å…¥ï¼‰
# ============================================================
echo ""
echo "ğŸ“‹ Test 4: Inserting test data..."

BATCH_ID="TEST_BATCH_$(date +%Y%m%d_%H%M%S)"
CYCLE_ID_1="TEST_CYCLE_001_$(date +%s)"
CYCLE_ID_2="TEST_CYCLE_002_$(date +%s)"
CYCLE_ID_3="TEST_CYCLE_003_$(date +%s)"

docker exec -i $MYSQL_CONTAINER mysql -u$MYSQL_USER -p$MYSQL_PASS $DATABASE <<EOF
-- æ’å…¥ 3 æ¡æµ‹è¯•æ•°æ®
INSERT INTO auto_test_case_catalog 
(batch_id, cycle_id, vehicle_id, shift_date, rule_version, 
 category, case_tags, severity, 
 trigger_timestamp, time_window_start, time_window_end, 
 triggered_rule_id, process_status, pack_base_path, created_at)
VALUES 
-- æµ‹è¯•æ•°æ® 1
('$BATCH_ID', '$CYCLE_ID_1', 'TEST_V001', CURDATE(), 'v1.0', 
 'CornerCase', '["TWIN_LIFT", "TEST"]', 'P1', 
 NOW(), NOW(), NOW() + INTERVAL 2 HOUR, 
 'rule_p1_twin_lift', 'PENDING', '/data/assets/test/', NOW()),

-- æµ‹è¯•æ•°æ® 2
('$BATCH_ID', '$CYCLE_ID_2', 'TEST_V002', CURDATE(), 'v1.0', 
 'CornerCase', '["OVERTAKE", "TEST"]', 'P1', 
 NOW(), NOW(), NOW() + INTERVAL 1 HOUR, 
 'rule_p1_overtake', 'PENDING', '/data/assets/test/', NOW()),

-- æµ‹è¯•æ•°æ® 3
('$BATCH_ID', '$CYCLE_ID_3', 'TEST_V003', CURDATE(), 'v1.0', 
 'CornerCase', '["EMERGENCY_STOP", "TEST"]', 'P1', 
 NOW(), NOW(), NOW() + INTERVAL 3 HOUR, 
 'rule_p1_emergency', 'PENDING', '/data/assets/test/', NOW());

SELECT 'âœ… Inserted 3 test records' AS status;

-- éªŒè¯æ’å…¥
SELECT 
    id,
    cycle_id,
    triggered_rule_id,
    process_status,
    pack_base_path
FROM auto_test_case_catalog
WHERE batch_id = '$BATCH_ID';
EOF

# ============================================================
# Test 5: éªŒè¯ PENDING çŠ¶æ€çš„æ•°æ®
# ============================================================
echo ""
echo "ğŸ“‹ Test 5: Verifying PENDING status records..."

docker exec -i $MYSQL_CONTAINER mysql -u$MYSQL_USER -p$MYSQL_PASS $DATABASE <<EOF
SELECT 
    COUNT(*) AS pending_count,
    CASE 
        WHEN COUNT(*) >= 3 THEN 'âœ… Test data ready'
        ELSE 'âŒ Insufficient test data'
    END AS status
FROM auto_test_case_catalog
WHERE batch_id = '$BATCH_ID'
  AND process_status = 'PENDING';
EOF

# ============================================================
# Test 6: æ‰‹åŠ¨è§¦å‘ DAG B
# ============================================================
echo ""
echo "ğŸ“‹ Test 6: Manually triggering DAG B..."

docker exec $AIRFLOW_CONTAINER airflow dags trigger asset_packing_dag && \
    echo "âœ… DAG B triggered successfully" || \
    echo "âŒ Failed to trigger DAG B"

# ============================================================
# Test 7: ç­‰å¾…å¹¶æŸ¥çœ‹æ‰§è¡ŒçŠ¶æ€
# ============================================================
echo ""
echo "ğŸ“‹ Test 7: Waiting for DAG B execution (30 seconds)..."
sleep 30

echo "Checking DAG B run status..."
docker exec $AIRFLOW_CONTAINER airflow dags list-runs -d asset_packing_dag --state running,success,failed | head -5

# ============================================================
# Test 8: æŸ¥çœ‹ cleanup_zombie_tasks æ—¥å¿—
# ============================================================
echo ""
echo "ğŸ“‹ Test 8: Checking cleanup_zombie_tasks logs..."
echo "(æœ€è¿‘ä¸€æ¬¡è¿è¡Œ)"

docker exec $AIRFLOW_CONTAINER bash -c "
    airflow tasks list asset_packing_dag | grep cleanup_zombie_tasks > /dev/null && \
    airflow dags list-runs -d asset_packing_dag --state success,running,failed --output table | head -5
" || echo "âš ï¸ Task not found yet"

# ============================================================
# Test 9: æŸ¥çœ‹ get_pending_assets æ—¥å¿—
# ============================================================
echo ""
echo "ğŸ“‹ Test 9: Checking get_pending_assets execution..."

docker exec $AIRFLOW_CONTAINER bash -c "
    LATEST_RUN=\$(airflow dags list-runs -d asset_packing_dag --output json | python3 -c 'import json, sys; runs = json.load(sys.stdin); print(runs[0][\"run_id\"] if runs else \"\")' 2>/dev/null || echo '')
    if [ ! -z \"\$LATEST_RUN\" ]; then
        echo \"Latest run ID: \$LATEST_RUN\"
        echo \"Fetching logs...\"
        airflow tasks logs asset_packing_dag get_pending_assets \$LATEST_RUN 2>/dev/null | tail -20 || echo 'âš ï¸ Logs not available yet'
    else
        echo 'âš ï¸ No runs found yet'
    fi
"

# ============================================================
# Test 10: æŸ¥çœ‹æ•°æ®åº“çŠ¶æ€å˜åŒ–
# ============================================================
echo ""
echo "ğŸ“‹ Test 10: Checking database status after DAG B execution..."

docker exec -i $MYSQL_CONTAINER mysql -u$MYSQL_USER -p$MYSQL_PASS $DATABASE <<EOF
SELECT 
    cycle_id,
    triggered_rule_id,
    process_status,
    pack_retry_count,
    SUBSTRING(pack_error_message, 1, 50) AS error_preview,
    pack_started_at,
    pack_completed_at
FROM auto_test_case_catalog
WHERE batch_id = '$BATCH_ID'
ORDER BY created_at;
EOF

# ============================================================
# Test 11: ç»Ÿè®¡ç»“æœ
# ============================================================
echo ""
echo "ğŸ“‹ Test 11: Summary statistics..."

docker exec -i $MYSQL_CONTAINER mysql -u$MYSQL_USER -p$MYSQL_PASS $DATABASE <<EOF
SELECT 
    process_status,
    COUNT(*) AS count
FROM auto_test_case_catalog
WHERE batch_id = '$BATCH_ID'
GROUP BY process_status;
EOF

# ============================================================
# Test 12: æ£€æŸ¥æ‰“åŒ…æœåŠ¡è°ƒç”¨ï¼ˆæŸ¥çœ‹æ—¥å¿—ä¸­æ˜¯å¦æœ‰æ‰“åŒ…æœåŠ¡ç›¸å…³ä¿¡æ¯ï¼‰
# ============================================================
echo ""
echo "ğŸ“‹ Test 12: Checking packing service calls in Airflow logs..."

docker logs --tail 50 $AIRFLOW_CONTAINER 2>&1 | grep -E "(PackingServiceClient|Packing started|pack_key)" | tail -10 || \
    echo "âš ï¸ No packing service logs found (may not have reached pack_assets task yet)"

# ============================================================
# å®Œæˆ
# ============================================================
echo ""
echo "=================================================="
echo "âœ… DAG B Test completed!"
echo ""
echo "ğŸ“ Next Steps:"
echo "  1. Review the test results above"
echo "  2. Check Airflow UI: http://localhost:8080"
echo "  3. View DAG B graph: http://localhost:8080/dags/asset_packing_dag/grid"
echo "  4. If pack_assets task is running, check its logs in Airflow UI"
echo ""
echo "ğŸ§¹ Cleanup test data (optional):"
echo "  docker exec -i $MYSQL_CONTAINER mysql -u$MYSQL_USER -p$MYSQL_PASS $DATABASE -e \"DELETE FROM auto_test_case_catalog WHERE batch_id = '$BATCH_ID';\""
echo ""
