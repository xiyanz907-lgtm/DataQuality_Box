# DAG B å¿«é€Ÿå¯åŠ¨æŒ‡å—

## ğŸš€ 5 åˆ†é’Ÿå¿«é€Ÿéƒ¨ç½²

### Step 1: åˆ›å»ºæ•°æ®åº“è¡¨ï¼ˆ2 åˆ†é’Ÿï¼‰

```bash
# è¿›å…¥ MySQL å®¹å™¨
docker exec -it deploy-mysql-1 mysql -u root -p

# åˆ‡æ¢æ•°æ®åº“ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…æ•°æ®åº“åï¼‰
USE your_database_name;

# å¤åˆ¶ç²˜è´´ä»¥ä¸‹ SQL å¹¶æ‰§è¡Œ
```

```sql
-- 1. åˆ›å»ºæ‰“åŒ…é˜Ÿåˆ—è¡¨
CREATE TABLE IF NOT EXISTS governance_asset_packing_queue (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    batch_id VARCHAR(100) NOT NULL,
    asset_id VARCHAR(100) NOT NULL,
    rule_id VARCHAR(100) NOT NULL,
    vehicle_id VARCHAR(50) NOT NULL,
    start_time DATETIME NOT NULL,
    end_time DATETIME NOT NULL,
    base_path VARCHAR(500) NOT NULL,
    status ENUM('PENDING', 'PROCESSING', 'POLLING', 'SUCCESS', 'FAILED', 'ABANDONED') DEFAULT 'PENDING',
    pack_key VARCHAR(200),
    pack_url VARCHAR(500),
    poll_count INT DEFAULT 0,
    error_message TEXT,
    retry_count INT DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    pack_started_at DATETIME,
    processed_at DATETIME,
    UNIQUE KEY uk_asset (batch_id, asset_id),
    INDEX idx_status_created (status, created_at),
    INDEX idx_status_updated (status, updated_at),
    INDEX idx_status_polling (status, poll_count),
    INDEX idx_batch_id (batch_id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

-- 2. å‡çº§ meta è¡¨
ALTER TABLE auto_test_case_catalog 
ADD COLUMN retry_count INT DEFAULT 0 
COMMENT 'æ‰“åŒ…é‡è¯•æ¬¡æ•°' 
AFTER process_status;

-- 3. éªŒè¯
DESC governance_asset_packing_queue;
DESC auto_test_case_catalog;
```

### Step 2: é‡å¯ Airflowï¼ˆ1 åˆ†é’Ÿï¼‰

```bash
cd /home/ubuntu/cactus_box/cactus-box/deploy
docker-compose restart airflow

# ç­‰å¾… 30 ç§’è®© Airflow å®Œå…¨å¯åŠ¨
sleep 30
```

### Step 3: éªŒè¯ DAGï¼ˆ1 åˆ†é’Ÿï¼‰

```bash
# æ£€æŸ¥ DAG A å’Œ DAG B
docker exec deploy-airflow-1 airflow dags list | grep -E "(governance_main_dag|asset_packing_dag)"

# é¢„æœŸè¾“å‡ºï¼š
# governance_main_dag    | ...  | True
# asset_packing_dag      | ...  | True

# æ£€æŸ¥ Dataset
docker exec deploy-airflow-1 airflow datasets list | grep governance_asset_packing_queue

# é¢„æœŸè¾“å‡ºï¼š
# mysql://datalog_mysql_conn/governance_asset_packing_queue
```

### Step 4: æ‰‹åŠ¨æµ‹è¯•ï¼ˆ1 åˆ†é’Ÿï¼‰

```bash
# 1. æ’å…¥æµ‹è¯•æ•°æ®
docker exec -i deploy-mysql-1 mysql -u root -p your_database_name <<EOF
INSERT INTO governance_asset_packing_queue 
(batch_id, asset_id, rule_id, vehicle_id, start_time, end_time, base_path, status)
VALUES 
('TEST_BATCH_001', 'TEST_ASSET_001', 'rule_p1_twin_lift', 'V001', 
 NOW(), NOW() + INTERVAL 2 HOUR, '/data/assets/test/', 'PENDING');
EOF

# 2. è§¦å‘ DAG B
docker exec deploy-airflow-1 airflow dags trigger asset_packing_dag

# 3. ç­‰å¾… 10 ç§’åæŸ¥çœ‹ç»“æœ
sleep 10

docker exec -i deploy-mysql-1 mysql -u root -p your_database_name -e \
"SELECT asset_id, status, retry_count, error_message FROM governance_asset_packing_queue WHERE asset_id = 'TEST_ASSET_001';"

# 4. æ¸…ç†æµ‹è¯•æ•°æ®
docker exec -i deploy-mysql-1 mysql -u root -p your_database_name -e \
"DELETE FROM governance_asset_packing_queue WHERE asset_id = 'TEST_ASSET_001';"
```

---

## âœ… éªŒè¯æ¸…å•

å®Œæˆä»¥ä¸Šæ­¥éª¤åï¼Œè¯·ç¡®è®¤ï¼š

- [ ] `governance_asset_packing_queue` è¡¨å·²åˆ›å»º
- [ ] `auto_test_case_catalog` è¡¨æœ‰ `retry_count` å­—æ®µ
- [ ] DAG A (`governance_main_dag`) åœ¨ Airflow UI ä¸­æ˜¾ç¤ºä¸º Active
- [ ] DAG B (`asset_packing_dag`) åœ¨ Airflow UI ä¸­æ˜¾ç¤ºä¸º Active
- [ ] Dataset `GOVERNANCE_ASSET_DATASET` åœ¨ Airflow UI ä¸­å¯è§
- [ ] æ‰‹åŠ¨æµ‹è¯•é€šè¿‡ï¼ˆæµ‹è¯•æ•°æ®è¢«æˆåŠŸå¤„ç†ï¼‰

---

## ğŸ‰ å®Œæˆï¼

ç°åœ¨å¯ä»¥ï¼š

1. **æŸ¥çœ‹ Airflow UI**: http://localhost:8080
   - å¯¼èˆªåˆ° `Datasets` é¡µé¢ï¼ŒæŸ¥çœ‹ `GOVERNANCE_ASSET_DATASET`
   - æŸ¥çœ‹ `asset_packing_dag` çš„ Schedule é…ç½®

2. **è§¦å‘å®Œæ•´æµç¨‹**:
   ```bash
   docker exec deploy-airflow-1 airflow dags trigger governance_main_dag
   ```
   
   é¢„æœŸï¼š
   - DAG A å®Œæˆåï¼Œè‡ªåŠ¨è§¦å‘ DAG B
   - P1 èµ„äº§è¢«è‡ªåŠ¨æ‰“åŒ…

3. **ç›‘æ§é˜Ÿåˆ—çŠ¶æ€**:
   ```sql
   SELECT status, COUNT(*) AS count
   FROM governance_asset_packing_queue
   GROUP BY status;
   ```

---

## ğŸ“š è¿›ä¸€æ­¥é˜…è¯»

- [å®Œæ•´éƒ¨ç½²æŒ‡å—](DAG_B_DEPLOYMENT_GUIDE.md)
- [å®æ–½æ€»ç»“](DAG_B_IMPLEMENTATION_SUMMARY.md)
- [ç›‘æ§ SQL](database/monitoring/asset_packing_monitor.sql)

---

## ğŸ†˜ é‡åˆ°é—®é¢˜ï¼Ÿ

### é—®é¢˜ 1: DAG æœªæ˜¾ç¤º
```bash
# æ£€æŸ¥æ—¥å¿—
docker logs deploy-airflow-1 | tail -50

# æ£€æŸ¥ DAG æ–‡ä»¶è¯­æ³•
docker exec deploy-airflow-1 python3 -m py_compile /opt/airflow/dags/asset_packing_dag.py
```

### é—®é¢˜ 2: Dataset æœªè§¦å‘
```bash
# æ£€æŸ¥ Dataset äº‹ä»¶
docker exec deploy-airflow-1 airflow datasets list-events

# æ‰‹åŠ¨è§¦å‘ DAG B æµ‹è¯•
docker exec deploy-airflow-1 airflow dags trigger asset_packing_dag
```

### é—®é¢˜ 3: æ‰“åŒ…æœåŠ¡è¿æ¥å¤±è´¥
```bash
# æµ‹è¯•æ‰“åŒ…æœåŠ¡
curl -X GET "https://mock.apipost.net/mock/34a21a/api/launcher/querySyncCacheResult?key=test" \
  -H "Authorization: Bearer YOUR_TOKEN"

# ä¿®æ”¹ plugins/services/packing_service.py ä¸­çš„ BASE_URL å’Œ AUTH_TOKEN
```

---

**éœ€è¦å¸®åŠ©ï¼Ÿ** è”ç³» data-governance@example.com
