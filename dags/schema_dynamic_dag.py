from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.providers.mysql.hooks.mysql import MySqlHook
import pendulum
import sys
import os

# è·¯å¾„é…ç½®
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
plugins_dir = os.path.join(project_root, 'plugins')
if plugins_dir not in sys.path:
    sys.path.append(plugins_dir)

# ======================================================
# 1. å¯¼å…¥ Schema 
# ======================================================
from schemas.kpi.cnt_cycles import CntCyclesSchema
from schemas.kpi.cnt_newcycles import CntNewCyclesSchema
from services.data_validator import run_pandera_validation

# ======================================================
# 2. é…ç½®åŒº 
# ======================================================
TABLES_TO_CHECK = [
    {
        "table_name": "kpi_data_db.cnt_cycles",   
        "schema_obj": CntCyclesSchema,            
        "task_id": "cnt_cycles",                  
        "batch_size": int(os.getenv("BATCH_SIZE", 50000))
    },
    {
        "table_name": "kpi_data_db.cnt_newcycles",
        "schema_obj": CntNewCyclesSchema,
        "task_id": "new_cnt_cycles",
        "batch_size": int(os.getenv("BATCH_SIZE", 50000))
    }
]

default_args = {
    'owner': 'box_admin',
    'start_date': pendulum.today('UTC').add(days=-1),
}

# ======================================================
# 3. é€šç”¨æ‰§è¡Œå‡½æ•° 
# ======================================================
def _validate_wrapper(table_name, schema_obj, task_id, batch_size, **context):
    conn_id = 'cactus_mysql_conn'
    hook = MySqlHook(mysql_conn_id=conn_id)

    # schema_watermark
    watermark_var_name = f"schema_watermark_{task_id}"
    
    # 1. è·å–ä¸Šæ¬¡å¤„ç†åˆ°çš„ ID
    last_id = int(Variable.get(watermark_var_name, default_var=60258))
    
    # 2. æŸ¥è¯¢è¯¥è¡¨å½“å‰æœ€å¤§ ID
    max_id_sql = f"SELECT MAX(id) FROM {table_name}"
    current_max_id = hook.get_first(max_id_sql)[0]
    
    if current_max_id is None:
        print(f"è¡¨ {table_name} ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
        return

    if current_max_id <= last_id:
        print(f"è¡¨ {table_name} æ²¡æœ‰æ–°æ•°æ® (Last: {last_id}, Max: {current_max_id})ï¼Œè·³è¿‡ã€‚")
        return

    # 3. è®¡ç®—æœ¬æ‰¹æ¬¡ç›®æ ‡ ID
    target_id = min(current_max_id, last_id + batch_size)
    print(f"[{task_id}] æœ¬æ¬¡æ ¡éªŒèŒƒå›´: ID ({last_id}, {target_id}]")

    # 4. æ‰§è¡Œæ ¡éªŒ
    result = run_pandera_validation(
        conn_id=conn_id,
        table_name=table_name,
        schema_obj=schema_obj,
        custom_where=f"id > {last_id} AND id <= {target_id}",
        sql_limit=None
    )
    
    # 5. æ›´æ–°æ°´ä½çº¿

    Variable.set(watermark_var_name, target_id)
    print(f"ğŸ”„ [{task_id}] æ°´ä½çº¿å·²æ›´æ–°ä¸º: {target_id}")

    if target_id < current_max_id:
        print(f"âš ï¸ è¿˜æœ‰è¿½èµ¶æ•°æ®ï¼Œä¸‹æ¬¡ç»§ç»­ã€‚")

    if result['status'] == 'FAILED':
        raise ValueError(f"[{table_name}] Validation Failed! Found {result['error_count']} errors.")

    elif result['status'] == 'SKIPPED':
        Variable.set(watermark_var_name, target_id)
        print(f"âš ï¸ [{task_id}] æ— åŒºé—´æ•°æ®ï¼Œå·²è·³è¿‡ã€‚")

    else:
        print(f"âœ… [{task_id}] Validation Succeeded for ID up to {target_id}.")

# ======================================================
# 4. åŠ¨æ€ç”Ÿæˆ DAG
# ======================================================
# with DAG(
#     'schema_quality_check_dynamic',
#     default_args=default_args,
#     schedule='*/30 * * * *',
#     catchup=False,
#     is_paused_upon_creation=True,
#     tags=['quality', 'schema', 'dynamic']
# ) as dag:

#     for config in TABLES_TO_CHECK:
#         PythonOperator(
#             task_id=f"check_{config['task_id']}", 
#             python_callable=_validate_wrapper,
#             op_kwargs={
#                 "table_name": config["table_name"],
#                 "schema_obj": config["schema_obj"],
#                 "task_id": config["task_id"],
#                 "batch_size": config.get("batch_size", int(os.getenv("BATCH_SIZE", 50000)))
#             }
#         )