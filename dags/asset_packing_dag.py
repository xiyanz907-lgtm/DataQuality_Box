"""
èµ„äº§æ‰“åŒ… DAG (DAG B)
äº‹ä»¶é©±åŠ¨ + æ•°æ®åº“é˜Ÿåˆ— + åƒµå°¸ä»»åŠ¡å¤„ç†

è§¦å‘æ–¹å¼ï¼šç”± DAG A é€šè¿‡ Dataset è§¦å‘
å·¥ä½œæµç¨‹ï¼š
1. æ¸…ç†åƒµå°¸ä»»åŠ¡ï¼ˆPROCESSING/POLLING è¶…æ—¶ 2 å°æ—¶ï¼‰
2. ä»é˜Ÿåˆ—è·å–å¾…å¤„ç†èµ„äº§ï¼ˆPENDINGï¼Œæ‰¹é‡ 50 æ¡ï¼‰
3. è°ƒç”¨æ‰“åŒ…æœåŠ¡ï¼ˆå¼‚æ­¥æ¥å£ï¼‰
4. è½®è¯¢æ‰“åŒ…çŠ¶æ€ï¼ˆæœ€å¤š 60 æ¬¡ï¼‰
5. æ›´æ–°å…ƒæ•°æ®è¡¨ï¼ˆprocess_status = PACKAGEDï¼‰
6. å‘é€å¤±è´¥æ±‡æ€»é‚®ä»¶ï¼ˆå¦‚æœ‰å¤±è´¥ä»»åŠ¡ï¼‰

Author: Data Governance Team
Date: 2026-02-02
"""
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.providers.mysql.hooks.mysql import MySqlHook
from airflow.utils.email import send_email
from datetime import datetime, timedelta
from typing import List, Dict, Any
import logging
import os

from plugins.datasets import GOVERNANCE_ASSET_DATASET
from plugins.services.packing_service import create_packing_client


# ============================================================
# é…ç½®å‚æ•°
# ============================================================
MYSQL_CONN_ID = 'qa_mysql_conn'
META_TABLE = 'auto_test_case_catalog'  # å•è¡¨æ–¹æ¡ˆï¼šåªä½¿ç”¨ meta è¡¨

BATCH_SIZE = 50  # æ¯æ¬¡å¤„ç†çš„èµ„äº§æ•°é‡
ZOMBIE_TIMEOUT_HOURS = 2  # åƒµå°¸ä»»åŠ¡è¶…æ—¶æ—¶é—´
MAX_RETRY_COUNT = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
BACKLOG_ALERT_THRESHOLD = 500  # é˜Ÿåˆ—ç§¯å‹å‘Šè­¦é˜ˆå€¼


# ============================================================
# Task 1: æ¸…ç†åƒµå°¸ä»»åŠ¡
# ============================================================
def cleanup_zombie_tasks(**context):
    """
    æ¸…ç†åƒµå°¸ä»»åŠ¡ï¼ˆPROCESSING/POLLING çŠ¶æ€è¶…æ—¶ 2 å°æ—¶ï¼‰
    
    é€»è¾‘ï¼š
    1. æ£€æµ‹ status IN ('PROCESSING', 'POLLING') ä¸” updated_at < NOW() - INTERVAL 2 HOUR
    2. å¦‚æœ retry_count < 3: é‡ç½®ä¸º PENDING
    3. å¦‚æœ retry_count >= 3: æ ‡è®°ä¸º ABANDONED
    """
    logger = logging.getLogger(__name__)
    hook = MySqlHook(mysql_conn_id=MYSQL_CONN_ID)
    
    # SQL: é‡ç½®å¯é‡è¯•çš„åƒµå°¸ä»»åŠ¡ï¼ˆå•è¡¨æ–¹æ¡ˆï¼‰
    reset_sql = f"""
        UPDATE {META_TABLE}
        SET process_status = 'PENDING',
            pack_error_message = CONCAT(IFNULL(pack_error_message, ''), ' [Zombie Reset]'),
            pack_retry_count = pack_retry_count + 1,
            updated_at = NOW()
        WHERE process_status IN ('PROCESSING', 'POLLING')
          AND updated_at < NOW() - INTERVAL {ZOMBIE_TIMEOUT_HOURS} HOUR
          AND pack_retry_count < {MAX_RETRY_COUNT}
    """
    
    # SQL: æ”¾å¼ƒè¶…è¿‡é‡è¯•æ¬¡æ•°çš„åƒµå°¸ä»»åŠ¡ï¼ˆå•è¡¨æ–¹æ¡ˆï¼‰
    abandon_sql = f"""
        UPDATE {META_TABLE}
        SET process_status = 'ABANDONED',
            pack_error_message = CONCAT(IFNULL(pack_error_message, ''), ' [Max Retries Exceeded]'),
            updated_at = NOW()
        WHERE process_status IN ('PROCESSING', 'POLLING')
          AND updated_at < NOW() - INTERVAL {ZOMBIE_TIMEOUT_HOURS} HOUR
          AND pack_retry_count >= {MAX_RETRY_COUNT}
    """
    
    try:
        # æ‰§è¡Œé‡ç½®
        reset_count = hook.run(reset_sql, handler=lambda cursor: cursor.rowcount)
        logger.info(f"â™»ï¸ Reset {reset_count} zombie tasks to PENDING")
        
        # æ‰§è¡Œæ”¾å¼ƒ
        abandon_count = hook.run(abandon_sql, handler=lambda cursor: cursor.rowcount)
        logger.info(f"ğŸ—‘ï¸ Abandoned {abandon_count} zombie tasks (max retries exceeded)")
        
        # æ¨é€ XCom
        context['ti'].xcom_push(key='zombie_reset_count', value=reset_count)
        context['ti'].xcom_push(key='zombie_abandon_count', value=abandon_count)
        
        # å‘Šè­¦æ£€æŸ¥
        if reset_count > 10 or abandon_count > 5:
            logger.warning(f"âš ï¸ High zombie task count! Reset: {reset_count}, Abandoned: {abandon_count}")
        
    except Exception as e:
        logger.error(f"âŒ Cleanup zombie tasks failed: {str(e)}")
        raise


# ============================================================
# Task 2: è·å–å¾…å¤„ç†èµ„äº§
# ============================================================
def get_pending_assets(**context):
    """
    ä»é˜Ÿåˆ—è·å–å¾…å¤„ç†èµ„äº§ï¼ˆæ‰¹é‡ 50 æ¡ï¼Œè¡Œé”é˜²æ­¢å¹¶å‘å†²çªï¼‰
    
    è¿”å›ï¼šList[Dict] åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
    """
    logger = logging.getLogger(__name__)
    hook = MySqlHook(mysql_conn_id=MYSQL_CONN_ID)
    
    # SQL: è·å–å¾…å¤„ç†ä»»åŠ¡ï¼ˆä½¿ç”¨è¡Œé”ï¼Œå•è¡¨æ–¹æ¡ˆï¼‰
    select_sql = f"""
        SELECT id, batch_id, cycle_id as asset_id, triggered_rule_id as rule_id,
               vehicle_id, time_window_start as start_time, time_window_end as end_time, 
               pack_base_path as base_path, pack_retry_count as retry_count
        FROM {META_TABLE}
        WHERE process_status = 'PENDING'
        ORDER BY created_at ASC
        LIMIT {BATCH_SIZE}
        FOR UPDATE SKIP LOCKED
    """
    
    # SQL: æ›´æ–°ä¸º PROCESSINGï¼ˆå•è¡¨æ–¹æ¡ˆï¼‰
    update_sql = f"""
        UPDATE {META_TABLE}
        SET process_status = 'PROCESSING',
            pack_started_at = NOW(),
            updated_at = NOW()
        WHERE id IN ({{}})
    """
    
    try:
        conn = hook.get_conn()
        cursor = conn.cursor()
        
        # å¼€å¯äº‹åŠ¡
        conn.autocommit = False
        
        # æŸ¥è¯¢å¾…å¤„ç†ä»»åŠ¡
        cursor.execute(select_sql)
        rows = cursor.fetchall()
        
        if not rows:
            logger.info("âœ… No pending assets, skipping packing")
            conn.rollback()
            context['ti'].xcom_push(key='pending_assets', value=[])
            return []
        
        # æ„é€ èµ„äº§åˆ—è¡¨
        assets = []
        asset_ids = []
        
        for row in rows:
            asset = {
                'queue_id': row[0],
                'batch_id': row[1],
                'asset_id': row[2],
                'rule_id': row[3],
                'vehicle_id': row[4],
                'start_time': row[5],
                'end_time': row[6],
                'base_path': row[7],
                'retry_count': row[8],
            }
            assets.append(asset)
            asset_ids.append(row[0])
        
        # æ‰¹é‡æ›´æ–°çŠ¶æ€ä¸º PROCESSING
        ids_str = ','.join(map(str, asset_ids))
        cursor.execute(update_sql.format(ids_str))
        
        # æäº¤äº‹åŠ¡
        conn.commit()
        
        logger.info(f"ğŸ“¦ Fetched {len(assets)} pending assets")
        logger.info(f"   Asset IDs: {asset_ids}")
        
        # æ¨é€ XCom
        context['ti'].xcom_push(key='pending_assets', value=assets)
        
        # é˜Ÿåˆ—ç§¯å‹å‘Šè­¦ï¼ˆå•è¡¨æ–¹æ¡ˆï¼‰
        cursor.execute(f"SELECT COUNT(*) FROM {META_TABLE} WHERE process_status = 'PENDING'")
        pending_count = cursor.fetchone()[0]
        if pending_count > BACKLOG_ALERT_THRESHOLD:
            logger.warning(f"âš ï¸ Queue backlog alert! {pending_count} assets pending")
        
        cursor.close()
        conn.close()
        
        return assets
        
    except Exception as e:
        logger.error(f"âŒ Get pending assets failed: {str(e)}")
        if conn:
            conn.rollback()
        raise


# ============================================================
# Task 3: åˆ†æ”¯åˆ¤æ–­
# ============================================================
def check_has_assets(**context):
    """
    æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†èµ„äº§
    
    è¿”å›ï¼š'pack_assets' æˆ– 'skip_packing'
    """
    assets = context['ti'].xcom_pull(task_ids='get_pending_assets', key='pending_assets')
    
    if assets:
        return 'pack_assets'
    else:
        return 'skip_packing'


# ============================================================
# Task 4: æ‰§è¡Œæ‰“åŒ…
# ============================================================
def pack_assets(**context):
    """
    æ‰¹é‡è°ƒç”¨æ‰“åŒ…æœåŠ¡ï¼ˆå¼‚æ­¥æ¥å£ + è½®è¯¢ï¼‰
    
    æµç¨‹ï¼š
    1. éå†èµ„äº§åˆ—è¡¨
    2. è°ƒç”¨ start_packing() è·å– pack_key
    3. æ›´æ–°çŠ¶æ€ä¸º POLLING
    4. è°ƒç”¨ wait_for_completion() ç­‰å¾…å®Œæˆ
    5. æ›´æ–°çŠ¶æ€ä¸º SUCCESS/FAILED
    """
    logger = logging.getLogger(__name__)
    assets = context['ti'].xcom_pull(task_ids='get_pending_assets', key='pending_assets')
    
    if not assets:
        logger.info("âœ… No assets to pack")
        return
    
    hook = MySqlHook(mysql_conn_id=MYSQL_CONN_ID)
    packing_client = create_packing_client(logger=logger)
    
    success_count = 0
    failed_count = 0
    failed_assets = []
    
    for asset in assets:
        queue_id = asset['queue_id']
        asset_id = asset['asset_id']
        vehicle_id = asset['vehicle_id']
        start_time = asset['start_time']
        end_time = asset['end_time']
        base_path = asset['base_path']
        retry_count = asset['retry_count']
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“¦ Processing asset: {asset_id}")
        logger.info(f"   Vehicle: {vehicle_id}, Time: {start_time} ~ {end_time}")
        logger.info(f"   Retry count: {retry_count}/{MAX_RETRY_COUNT}")
        
        try:
            # 1. å¯åŠ¨æ‰“åŒ…ä»»åŠ¡
            success, pack_key, error = packing_client.start_packing(
                vehicle_id=vehicle_id,
                start_time=start_time,
                end_time=end_time,
                base_path=base_path
            )
            
            if not success:
                # æ‰“åŒ…å¯åŠ¨å¤±è´¥
                logger.error(f"âŒ Start packing failed: {error}")
                _update_queue_status(hook, queue_id, 'FAILED', error, retry_count)
                failed_count += 1
                failed_assets.append({'asset_id': asset_id, 'error': error})
                continue
            
            # 2. æ›´æ–°çŠ¶æ€ä¸º POLLINGï¼ˆå•è¡¨æ–¹æ¡ˆï¼‰
            update_sql = f"""
                UPDATE {META_TABLE}
                SET process_status = 'POLLING',
                    pack_key = %s,
                    pack_poll_count = 0,
                    updated_at = NOW()
                WHERE id = %s
            """
            hook.run(update_sql, parameters=(pack_key, queue_id))
            logger.info(f"ğŸ” Polling started, pack_key: {pack_key}")
            
            # 3. ç­‰å¾…æ‰“åŒ…å®Œæˆ
            complete, error = packing_client.wait_for_completion(pack_key)
            
            if complete:
                # æ‰“åŒ…æˆåŠŸ
                logger.info(f"âœ… Packing completed for asset: {asset_id}")
                _update_queue_status(hook, queue_id, 'SUCCESS', None, retry_count, pack_key)
                success_count += 1
            else:
                # æ‰“åŒ…å¤±è´¥ï¼ˆè¶…æ—¶æˆ–æŸ¥è¯¢å¤±è´¥ï¼‰
                logger.error(f"âŒ Packing failed: {error}")
                _update_queue_status(hook, queue_id, 'FAILED', error, retry_count)
                failed_count += 1
                failed_assets.append({'asset_id': asset_id, 'error': error})
                
        except Exception as e:
            # å¼‚å¸¸æ•è·
            logger.error(f"ğŸ’¥ Unexpected error for asset {asset_id}: {str(e)}")
            _update_queue_status(hook, queue_id, 'FAILED', str(e), retry_count)
            failed_count += 1
            failed_assets.append({'asset_id': asset_id, 'error': str(e)})
    
    # ç»Ÿè®¡ç»“æœ
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š Packing Summary:")
    logger.info(f"   âœ… Success: {success_count}")
    logger.info(f"   âŒ Failed: {failed_count}")
    logger.info(f"   ğŸ“¦ Total: {len(assets)}")
    
    # æ¨é€ XCom
    context['ti'].xcom_push(key='success_count', value=success_count)
    context['ti'].xcom_push(key='failed_count', value=failed_count)
    context['ti'].xcom_push(key='failed_assets', value=failed_assets)


def _update_queue_status(hook, queue_id, status, error_msg, retry_count, pack_key=None):
    """
    æ›´æ–°é˜Ÿåˆ—çŠ¶æ€ï¼ˆå•è¡¨æ–¹æ¡ˆï¼‰
    
    Args:
        hook: MySqlHook
        queue_id: é˜Ÿåˆ—è®°å½•IDï¼ˆmeta è¡¨çš„ idï¼‰
        status: æ–°çŠ¶æ€
        error_msg: é”™è¯¯ä¿¡æ¯
        retry_count: å½“å‰é‡è¯•æ¬¡æ•°
        pack_key: æ‰“åŒ…ä»»åŠ¡Key
    """
    # å¦‚æœå¤±è´¥ä¸”è¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œæ ‡è®°ä¸º ABANDONED
    if status == 'FAILED' and retry_count >= MAX_RETRY_COUNT:
        status = 'ABANDONED'
        error_msg = f"{error_msg} [Max Retries Exceeded]"
    
    # å¦‚æœå¤±è´¥ä½†è¿˜å¯ä»¥é‡è¯•ï¼Œé‡ç½®ä¸º PENDING
    if status == 'FAILED' and retry_count < MAX_RETRY_COUNT:
        status = 'PENDING'
        retry_count += 1
    
    # å•è¡¨æ–¹æ¡ˆï¼šæ›´æ–° process_status
    update_sql = f"""
        UPDATE {META_TABLE}
        SET process_status = %s,
            pack_error_message = %s,
            pack_retry_count = %s,
            pack_poll_count = pack_poll_count + 1,
            updated_at = NOW(),
            pack_completed_at = CASE WHEN %s IN ('PACKAGED', 'ABANDONED') THEN NOW() ELSE NULL END
        WHERE id = %s
    """
    
    # æ³¨æ„ï¼šSUCCESS çŠ¶æ€æ˜ å°„ä¸º PACKAGED
    final_status = 'PACKAGED' if status == 'SUCCESS' else status
    hook.run(update_sql, parameters=(final_status, error_msg, retry_count, final_status, queue_id))


# ============================================================
# Task 5: éªŒè¯æ‰“åŒ…ç»“æœï¼ˆå•è¡¨æ–¹æ¡ˆï¼šä¸éœ€è¦åŒæ­¥è¡¨ï¼‰
# ============================================================
def validate_packing_results(**context):
    """
    éªŒè¯æ‰“åŒ…ç»“æœï¼ˆå•è¡¨æ–¹æ¡ˆï¼šçŠ¶æ€å·²åœ¨ pack_assets ä¸­æ›´æ–°ï¼‰
    
    é€»è¾‘ï¼š
    1. ç»Ÿè®¡æœ¬æ¬¡è¿è¡Œçš„æ‰“åŒ…ç»“æœ
    2. è®°å½•æ—¥å¿—
    """
    logger = logging.getLogger(__name__)
    hook = MySqlHook(mysql_conn_id=MYSQL_CONN_ID)
    
    # æŸ¥è¯¢æœ¬æ¬¡è¿è¡Œçš„ç»“æœç»Ÿè®¡ï¼ˆæœ€è¿‘ 1 å°æ—¶ï¼‰
    stats_sql = f"""
        SELECT 
            process_status,
            COUNT(*) as count
        FROM {META_TABLE}
        WHERE pack_completed_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
        GROUP BY process_status
    """
    
    try:
        conn = hook.get_conn()
        cursor = conn.cursor()
        cursor.execute(stats_sql)
        rows = cursor.fetchall()
        
        if not rows:
            logger.info("âœ… No assets processed in the last hour")
            return
        
        # è®°å½•ç»Ÿè®¡ç»“æœ
        logger.info("ğŸ“Š Packing Results Summary (last 1 hour):")
        for row in rows:
            status, count = row
            logger.info(f"   {status}: {count} assets")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.error(f"âŒ Validation failed: {str(e)}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ DAG ç»§ç»­æ‰§è¡Œ
        pass


# ============================================================
# Task 6: å‘é€å¤±è´¥æ±‡æ€»é‚®ä»¶
# ============================================================
def send_failure_summary(**context):
    """
    å‘é€å¤±è´¥æ±‡æ€»é‚®ä»¶ï¼ˆå¦‚æœ‰å¤±è´¥ä»»åŠ¡ï¼‰
    
    å†…å®¹ï¼š
    - å¤±è´¥æ•°é‡
    - å¤±è´¥èµ„äº§åˆ—è¡¨
    - é”™è¯¯ä¿¡æ¯
    """
    logger = logging.getLogger(__name__)
    
    failed_count = context['ti'].xcom_pull(task_ids='pack_assets', key='failed_count') or 0
    failed_assets = context['ti'].xcom_pull(task_ids='pack_assets', key='failed_assets') or []
    
    if failed_count == 0:
        logger.info("âœ… No failures, skipping email")
        return
    
    # æ„é€ é‚®ä»¶å†…å®¹
    subject = f"[Data Governance] Asset Packing Failures - {failed_count} Assets"
    
    body = f"""
<h2>èµ„äº§æ‰“åŒ…å¤±è´¥æ±‡æ€»</h2>
<p><strong>å¤±è´¥æ•°é‡ï¼š</strong> {failed_count}</p>
<p><strong>DAG è¿è¡Œæ—¶é—´ï¼š</strong> {context['execution_date']}</p>

<h3>å¤±è´¥èµ„äº§åˆ—è¡¨ï¼š</h3>
<table border="1" cellpadding="5" cellspacing="0">
    <tr>
        <th>Asset ID</th>
        <th>é”™è¯¯ä¿¡æ¯</th>
    </tr>
"""
    
    for asset in failed_assets:
        body += f"""
    <tr>
        <td>{asset['asset_id']}</td>
        <td>{asset['error']}</td>
    </tr>
"""
    
    body += """
</table>

<p><strong>å»ºè®®æ“ä½œï¼š</strong></p>
<ul>
    <li>æ£€æŸ¥æ‰“åŒ…æœåŠ¡æ˜¯å¦æ­£å¸¸</li>
    <li>æŸ¥çœ‹é˜Ÿåˆ—è¡¨ä¸­ FAILED/ABANDONED è®°å½•</li>
    <li>å¦‚éœ€é‡è¯•ï¼Œæ‰‹åŠ¨å°† status æ”¹å› PENDING</li>
</ul>
"""
    
    try:
        send_email(
            to=[os.getenv('ALERT_EMAIL_TO', 'xiyan.zhou@westwell-lab.com')],
            subject=subject,
            html_content=body
        )
        logger.info(f"ğŸ“§ Failure summary email sent ({failed_count} assets)")
        
    except Exception as e:
        logger.error(f"âŒ Send email failed: {str(e)}")


# ============================================================
# Task 7: è·³è¿‡æ‰“åŒ…ï¼ˆæ— ä»»åŠ¡æ—¶æ‰§è¡Œï¼‰
# ============================================================
def skip_packing(**context):
    """æ— ä»»åŠ¡æ—¶çš„å ä½ä»»åŠ¡"""
    logger = logging.getLogger(__name__)
    logger.info("âœ… No assets to pack, skipping")


# ============================================================
# DAG å®šä¹‰
# ============================================================
default_args = {
    'owner': 'data-governance',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'email': ['xiyan.zhou@westwell-lab.com'],
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='asset_packing_dag',
    default_args=default_args,
    description='Asset Packing DAG (Event-Driven by Dataset)',
    schedule=[GOVERNANCE_ASSET_DATASET],  # Dataset é©±åŠ¨
    start_date=datetime(2026, 1, 1),
    catchup=False,
    max_active_runs=1,  # é˜²æ­¢å¹¶å‘
    tags=['governance', 'asset-packing', 'dag-b'],
) as dag:
    
    # Task 1: æ¸…ç†åƒµå°¸ä»»åŠ¡
    cleanup_zombies = PythonOperator(
        task_id='cleanup_zombie_tasks',
        python_callable=cleanup_zombie_tasks,
    )
    
    # Task 2: è·å–å¾…å¤„ç†èµ„äº§
    get_assets = PythonOperator(
        task_id='get_pending_assets',
        python_callable=get_pending_assets,
    )
    
    # Task 3: åˆ†æ”¯åˆ¤æ–­
    branch = BranchPythonOperator(
        task_id='check_has_assets',
        python_callable=check_has_assets,
    )
    
    # Task 4: æ‰§è¡Œæ‰“åŒ…
    pack = PythonOperator(
        task_id='pack_assets',
        python_callable=pack_assets,
    )
    
    # Task 5: è·³è¿‡æ‰“åŒ…
    skip = PythonOperator(
        task_id='skip_packing',
        python_callable=skip_packing,
    )
    
    # Task 6: éªŒè¯æ‰“åŒ…ç»“æœ
    validate_results = PythonOperator(
        task_id='validate_results',
        python_callable=validate_packing_results,
        trigger_rule='none_failed',
    )
    
    # Task 7: å‘é€å¤±è´¥æ±‡æ€»é‚®ä»¶
    send_summary = PythonOperator(
        task_id='send_failure_summary',
        python_callable=send_failure_summary,
        trigger_rule='none_failed',
    )
    
    # å®šä¹‰ä»»åŠ¡ä¾èµ–
    cleanup_zombies >> get_assets >> branch
    branch >> [pack, skip]
    [pack, skip] >> validate_results >> send_summary
