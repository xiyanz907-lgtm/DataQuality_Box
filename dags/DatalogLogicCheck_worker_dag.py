"""
DQ v1 Worker DAG - Ground Truth Validation

èŒè´£ï¼š
1) æ¥æ”¶ Controller ä¼ é€’çš„å‚æ•°ï¼ˆshift_date, vehicle_idsï¼‰
2) å››æ­¥éªŒè¯é€»è¾‘ï¼š
   Step 1: ä» MySQL æå–ç›®æ ‡è½¦è¾†çš„ä½œä¸šå£°æ˜ï¼ˆsubtarget_vehicle_cycleï¼‰
   Step 2: ä» InfluxDB è·å–ç‰©ç†å±‚çœŸç›¸ï¼ˆå®é™…ä½ç½®/é€Ÿåº¦ï¼‰
   Step 3: ä»åœ°å›¾æœåŠ¡è·å–è¯­ä¹‰å±‚çœŸç›¸ï¼ˆé“è·¯ç±»å‹ï¼‰
   Step 4: ä½¿ç”¨ Pandera éªŒè¯è§„åˆ™ï¼Œå†™å…¥ç»“æœè¡¨

è¾“å…¥å‚æ•°ï¼ˆdag_run.confï¼‰ï¼š
- shift_date: str (e.g., "2025-11-02")
- vehicle_list: list[str] (e.g., ["AT01", "AT02", "AT05"])

å…³é”®æŠ€æœ¯ï¼š
- Polars: é«˜æ€§èƒ½æ•°æ®è½¬æ¢ï¼ˆUnpivot + æ—¶é—´è§£æï¼‰
- InfluxDB: ç‰©ç†å±‚çœŸç›¸ï¼ˆGPS ä½ç½®/é€Ÿåº¦ï¼‰
- Map API: è¯­ä¹‰å±‚çœŸç›¸ï¼ˆé“è·¯ç±»å‹ï¼‰
- Pandera: æ•°æ®è´¨é‡éªŒè¯
"""

import os
import sys
import logging
from datetime import timedelta
from typing import Dict, List, Optional
from io import StringIO

import pendulum
import polars as pl
import pandera.polars as pa
from pandera import Column, DataFrameSchema, Check

from airflow import DAG
from airflow.decorators import task
from airflow.providers.mysql.hooks.mysql import MySqlHook
from airflow.exceptions import AirflowException

# ç»Ÿä¸€å¤ç”¨é¡¹ç›®å†…çš„ Connection ID é…ç½®
from services.config import CONN_ID_DATALOG as CONFIG_CONN_ID_DATALOG

# ç¡®ä¿ plugins è·¯å¾„å¯å¯¼å…¥
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
plugins_dir = os.path.join(project_root, "plugins")
if plugins_dir not in sys.path:
    sys.path.append(plugins_dir)

# Ground Truth éªŒè¯å·¥å…·
from dq_lib.ground_truth_utils import InfluxClient, MapClient

# -----------------------------
# é…ç½®åŒº
# -----------------------------
DAG_ID = "DatalogLogicCheck_worker"

# MySQL è¿æ¥ï¼ˆDataLog å¼€å‘åº“ï¼‰
CONN_ID_DATALOG = os.getenv("DATALOG_CONN_ID", CONFIG_CONN_ID_DATALOG)

# è¡¨åï¼ˆåº“åå›ºå®šä¸º dagster_pipelinesï¼‰
TBL_SUBTARGET = "dagster_pipelines.subtarget_vehicle_cycle"
TBL_RESULT = "dagster_pipelines.datalog_logic_check_result"
TBL_FAILURE_DETAIL = "dagster_pipelines.datalog_logic_check_failed_detail"

# InfluxDB é…ç½®
INFLUX_URL = os.getenv("INFLUX_URL", "http://10.105.66.20:8086")
INFLUX_TOKEN = os.getenv("INFLUX_TOKEN", "your_token_here")
INFLUX_ORG = os.getenv("INFLUX_ORG", "your_org")
INFLUX_BUCKET = os.getenv("INFLUX_BUCKET", "vehicle_telemetry")

# Map API é…ç½®
MAP_API_URL = os.getenv("MAP_API_URL", "http://10.105.66.20:1234/api/v1/annotate/batch")
MAP_PORT = os.getenv("MAP_PORT", "AQCTMap_20251121V1.0")

# éªŒè¯è§„åˆ™å¸¸é‡
MAX_SPEED_FOR_STATIONARY = 0.5  # é™æ­¢é˜ˆå€¼ï¼ˆm/sï¼‰

default_args = {
    "owner": "data_engineering",
    "depends_on_past": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=3),
}


# -----------------------------
# è¾…åŠ©å‡½æ•°ï¼šè¿ç»­ 'To QC' ç­›é€‰
# -----------------------------
def _apply_consecutive_qc_filter(df_all: pl.DataFrame, df_valid: pl.DataFrame, logger) -> pl.DataFrame:
    """
    åº”ç”¨è¿ç»­ 'To QC' ç­›é€‰è§„åˆ™ï¼ˆæ”¹è¿›ç‰ˆï¼‰ï¼š
    - åŸºäºæ‰€æœ‰ 'To QC'ï¼ˆåŒ…æ‹¬ align_time ä¸ºç©ºçš„ï¼‰åˆ¤æ–­è¿ç»­æ€§
    - ä½†åªä¿ç•™æœ‰ timestamp çš„è®°å½•
    - å¦‚æœä¸€ä¸ª cycle_id æœ‰è¿ç»­çš„ 'To QC'ï¼ˆç´¢å¼•ç›¸é‚»ï¼‰ï¼Œåªä¿ç•™æœ‰å€¼çš„æœ€åä¸€ä¸ª
    - å¦‚æœéƒ½ä¸è¿ç»­ï¼Œä¿ç•™æ‰€æœ‰æœ‰å€¼çš„è®°å½•
    
    Args:
        df_all: æ‰€æœ‰ 'To QC' è®°å½•ï¼ˆç”¨äºåˆ¤æ–­è¿ç»­æ€§ï¼‰
        df_valid: åªåŒ…å«æœ‰ timestamp çš„ 'To QC' è®°å½•ï¼ˆç”¨äºæœ€ç»ˆè¾“å‡ºï¼‰
        logger: Airflow logger
    
    Returns:
        è¿‡æ»¤åçš„ DataFrame
    """
    import pandas as pd
    
    # è½¬æ¢ä¸º Pandas æ–¹ä¾¿åˆ†ç»„å¤„ç†
    df_all_pandas = df_all.to_pandas()
    df_valid_pandas = df_valid.to_pandas()
    
    result_records = []
    
    for cycle_id in df_all_pandas['cycle_id'].unique():
        # ä» df_all è·å–æ‰€æœ‰ 'To QC' çš„ç´¢å¼•ï¼ˆåŒ…æ‹¬ align_time ä¸ºç©ºçš„ï¼‰
        all_indices = sorted(df_all_pandas[df_all_pandas['cycle_id'] == cycle_id]['idx'].tolist())
        
        # ä» df_valid è·å–æœ‰ timestamp çš„è®°å½•
        valid_group = df_valid_pandas[df_valid_pandas['cycle_id'] == cycle_id]
        
        if len(valid_group) == 0:
            # æ²¡æœ‰æœ‰æ•ˆè®°å½•ï¼Œè·³è¿‡
            continue
        
        valid_indices = sorted(valid_group['idx'].tolist())
        
        logger.info(f"[Consecutive Filter] cycle_id={cycle_id}, all QC indices={all_indices}, valid indices={valid_indices}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­ç´¢å¼•ï¼ˆåŸºäºæ‰€æœ‰ 'To QC'ï¼ŒåŒ…æ‹¬ align_time ä¸ºç©ºçš„ï¼‰
        has_consecutive = False
        if len(all_indices) >= 2:
            for i in range(len(all_indices) - 1):
                if all_indices[i+1] - all_indices[i] == 1:
                    has_consecutive = True
                    break
        
        if has_consecutive:
            # æ‰¾å‡ºæœ‰å€¼çš„æœ€å¤§ç´¢å¼•ï¼ˆä» valid_indices ä¸­é€‰æ‹©æœ€å¤§çš„ï¼‰
            max_valid_idx = max(valid_indices)
            filtered = valid_group[valid_group['idx'] == max_valid_idx]
            logger.info(f"[Consecutive Filter] cycle_id={cycle_id}: Has consecutive, keeping only valid idx={max_valid_idx}")
        else:
            # éƒ½ä¸è¿ç»­ï¼Œä¿ç•™æ‰€æœ‰æœ‰å€¼çš„è®°å½•
            filtered = valid_group
            logger.info(f"[Consecutive Filter] cycle_id={cycle_id}: No consecutive, keeping all {len(valid_indices)} valid records")
        
        result_records.append(filtered)
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    if result_records:
        df_result_pandas = pd.concat(result_records, ignore_index=True)
        df_result = pl.from_pandas(df_result_pandas)
        return df_result
    else:
        # è¿”å›ç©º DataFrameï¼ˆä¿æŒç›¸åŒçš„ schemaï¼‰
        return df.head(0)


with DAG(
    dag_id=DAG_ID,
    schedule_interval=None,  # ç”± Controller è§¦å‘
    start_date=pendulum.today("UTC").add(days=-1),
    catchup=False,
    default_args=default_args,
    tags=["dq", "worker", "ground_truth", "validation"],
    is_paused_upon_creation=False,
) as dag:

    @task
    def extract_claims(**context) -> Optional[str]:
        """
        Step 1: ä» MySQL æå–ç›®æ ‡è½¦è¾†çš„ä½œä¸šå£°æ˜
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. ä½¿ç”¨ shift_date å’Œ vehicle_ids åŠ¨æ€æ„å»º SQL WHERE æ¡ä»¶
        2. æå– subtask_type_1~8 å’Œ ALIGN_STA_TIME_1~8_SUBTASK åˆ—
        3. ä½¿ç”¨ Polars Unpivot å°†å®½è¡¨è½¬ä¸ºé•¿è¡¨ï¼ˆ1 è¡Œ/å­ä»»åŠ¡ï¼‰
        4. æ—¶é—´è½¬æ¢ï¼šå­—ç¬¦ä¸² -> Datetime -> Unix Timestamp (Int64)
        5. è¿‡æ»¤ï¼šsubtask_type == "To QC" AND timestamp IS NOT NULL
        
        Args:
            shift_date: ç­æ¬¡æ—¥æœŸ (e.g., "2025-11-02")
            vehicle_ids: è½¦è¾† ID åˆ—è¡¨ (e.g., ["AT01", "AT02"])
        
        Returns:
            str: Polars DataFrame åºåˆ—åŒ–ä¸º JSONï¼ˆä¾›ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ï¼‰
        """
        logger = logging.getLogger("airflow.task")
        
        # ä» dag_run.conf è¯»å–å‚æ•°
        dag_run = context['dag_run']
        shift_date = dag_run.conf.get('target_shift_date', '')
        vehicle_ids = dag_run.conf.get('vehicle_list', [])
        
        logger.info(f"[Step 1] Extracting claims for shift_date={shift_date}, vehicles={vehicle_ids}")
        
        if not vehicle_ids:
            logger.warning("[Step 1] Empty vehicle_ids, skipping.")
            return None
        
        hook = MySqlHook(mysql_conn_id=CONN_ID_DATALOG)
        
        # æ„å»ºåŠ¨æ€ SQLï¼ˆä½¿ç”¨ WHERE IN å­å¥ï¼‰
        # æ³¨æ„ï¼švehicle_ids å·²ç»æ˜¯åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸º SQL IN æ ¼å¼
        vehicle_ids_sql = ", ".join([f"'{vid}'" for vid in vehicle_ids])
        
        sql = f"""
        SELECT
            vehicle_id,
            TRACTOR_CYCLE_ID as cycle_id,
            SUBTASK_TYPE_1, ALIGN_STA_TIME_1_SUBTASK,
            SUBTASK_TYPE_2, ALIGN_STA_TIME_2_SUBTASK,
            SUBTASK_TYPE_3, ALIGN_STA_TIME_3_SUBTASK,
            SUBTASK_TYPE_4, ALIGN_STA_TIME_4_SUBTASK,
            SUBTASK_TYPE_5, ALIGN_STA_TIME_5_SUBTASK,
            SUBTASK_TYPE_6, ALIGN_STA_TIME_6_SUBTASK,
            SUBTASK_TYPE_7, ALIGN_STA_TIME_7_SUBTASK,
            SUBTASK_TYPE_8, ALIGN_STA_TIME_8_SUBTASK
        FROM {TBL_SUBTARGET}
        WHERE shift_date = '{shift_date}'
          AND vehicle_id IN ({vehicle_ids_sql})
        """
        
        logger.info(f"[Step 1] SQL Query:\n{sql}")
        
        # ä½¿ç”¨ Pandas è¯»å–ï¼Œç„¶åè½¬ä¸º Polarsï¼ˆAirflow MySQL Hook ä¸ç›´æ¥æ”¯æŒ Polarsï¼‰
        df_pandas = hook.get_pandas_df(sql)
        
        if df_pandas.empty:
            logger.warning(f"[Step 1] No data found for shift_date={shift_date}, vehicles={vehicle_ids}")
            return None
        
        logger.info(f"[Step 1] Fetched {len(df_pandas)} rows from MySQL")
        
        # è½¬æ¢ä¸º Polars
        df = pl.from_pandas(df_pandas)
        
        # Unpivot/Melt: å°† 8 å¯¹ (SUBTASK_TYPE_N, ALIGN_STA_TIME_N_SUBTASK) è½¬ä¸ºé•¿è¡¨
        # ç›®æ ‡ï¼šæ¯è¡Œä¸€ä¸ªå­ä»»åŠ¡
        # æ³¨æ„ï¼šPolars 0.19.x ä½¿ç”¨ meltï¼Œ0.20+ ä½¿ç”¨ unpivot
        df_long = df.melt(
            id_vars=["vehicle_id", "cycle_id"],
            value_vars=[f"SUBTASK_TYPE_{i}" for i in range(1, 9)],
            variable_name="subtask_index",
            value_name="subtask_type",
        ).with_columns(
            # æå–æ•°å­—ç´¢å¼•ï¼ˆä¾‹å¦‚ "SUBTASK_TYPE_3" -> 3ï¼‰
            pl.col("subtask_index").str.extract(r"(\d+)$", 1).cast(pl.Int32).alias("idx")
        )
        
        # å°†å¯¹åº”çš„æ—¶é—´åˆ— Join å›æ¥
        # æ„å»ºæ—¶é—´åˆ—çš„é•¿è¡¨
        df_time = df.melt(
            id_vars=["vehicle_id", "cycle_id"],
            value_vars=[f"ALIGN_STA_TIME_{i}_SUBTASK" for i in range(1, 9)],
            variable_name="time_index",
            value_name="align_time_str",
        ).with_columns(
            pl.col("time_index").str.extract(r"(\d+)", 1).cast(pl.Int32).alias("idx")
        )
        
        # Join
        df_merged = df_long.join(
            df_time.select(["vehicle_id", "cycle_id", "idx", "align_time_str"]),
            on=["vehicle_id", "cycle_id", "idx"],
            how="left",
        )
        
        # ğŸ” è°ƒè¯•ï¼šæŸ¥çœ‹ melt åçš„æ•°æ®
        logger.info(f"[DEBUG] df_merged shape: {df_merged.shape}")
        logger.info(f"[DEBUG] df_merged columns: {df_merged.columns}")
        logger.info(f"[DEBUG] subtask_type unique values: {df_merged['subtask_type'].unique().to_list()}")
        logger.info(f"[DEBUG] Sample rows (first 3):\n{df_merged.head(3)}")
        
        # æ—¶é—´è½¬æ¢ï¼šå­—ç¬¦ä¸² -> Datetime -> Unix Timestamp (Int64, ç§’)
        # MySQL æ ¼å¼: '2025-12-21 09:32:02'
        df_merged = df_merged.with_columns(
            pl.col("align_time_str")
            .str.strptime(pl.Datetime, format="%Y-%m-%d %H:%M:%S", strict=False)
            .dt.epoch(time_unit="s")
            .cast(pl.Int64)
            .alias("unix_timestamp")
        )
        
        # ğŸ” è°ƒè¯•ï¼šæŸ¥çœ‹æ—¶é—´è½¬æ¢åçš„æ•°æ®
        logger.info(f"[DEBUG] Non-null unix_timestamp count: {df_merged.filter(pl.col('unix_timestamp').is_not_null()).shape[0]}")
        logger.info(f"[DEBUG] 'To QC' count: {df_merged.filter(pl.col('subtask_type') == 'To QC').shape[0]}")
        
        # å…ˆç­›é€‰æ‰€æœ‰ 'To QC'ï¼ˆåŒ…æ‹¬ align_time ä¸ºç©ºçš„ï¼Œç”¨äºåˆ¤æ–­è¿ç»­æ€§ï¼‰
        df_all_qc = df_merged.filter(pl.col("subtask_type") == "To QC")
        
        # å†ç­›é€‰æœ‰ timestamp çš„ 'To QC'
        df_valid_qc = df_merged.filter(
            (pl.col("subtask_type") == "To QC") & (pl.col("unix_timestamp").is_not_null())
        )
        
        logger.info(f"[Step 1] Found {len(df_all_qc)} 'To QC' tasks (including null timestamps)")
        logger.info(f"[Step 1] Found {len(df_valid_qc)} 'To QC' tasks with valid timestamps")
        
        if df_valid_qc.is_empty():
            logger.warning("[Step 1] No valid 'To QC' tasks found after filtering")
            return None
        
        # åº”ç”¨è¿ç»­ 'To QC' ç­›é€‰è§„åˆ™
        # è§„åˆ™ï¼šåŸºäºæ‰€æœ‰ 'To QC' åˆ¤æ–­è¿ç»­æ€§ï¼Œä½†åªä¿ç•™æœ‰ timestamp çš„è®°å½•
        df_filtered = _apply_consecutive_qc_filter(df_all_qc, df_valid_qc, logger)
        
        # é€‰æ‹©éœ€è¦çš„åˆ—
        df_filtered = df_filtered.select([
            "vehicle_id",
            "cycle_id",
            "unix_timestamp",
        ])
        
        logger.info(f"[Step 1] After consecutive filter: {len(df_filtered)} records")
        
        if df_filtered.is_empty():
            logger.warning("[Step 1] No records after consecutive filter")
            return None
        
        # åºåˆ—åŒ–ä¸º JSONï¼ˆä¾› XCom ä¼ é€’ï¼‰
        return df_filtered.write_json()

    @task
    def fetch_physical_truth(claims_json: Optional[str]) -> Optional[str]:
        """
        Step 2: ä» InfluxDB è·å–ç‰©ç†å±‚çœŸç›¸ï¼ˆå®é™…ä½ç½®/é€Ÿåº¦ï¼‰
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. ååºåˆ—åŒ– Step 1 çš„ DataFrame
        2. éå†æ¯è¡Œï¼ŒæŸ¥è¯¢ InfluxDBï¼š[unix_timestamp - 1s, unix_timestamp + 1s]
        3. èšåˆï¼ˆMEANï¼‰ä»¥é™å™ª
        4. å°† actual_x, actual_y, actual_speed è¿½åŠ åˆ° DataFrame
        
        Args:
            claims_json: Step 1 è¾“å‡ºçš„ JSON å­—ç¬¦ä¸²
        
        Returns:
            str: æ›´æ–°åçš„ DataFrame (JSON)
        """
        logger = logging.getLogger("airflow.task")
        logger.info("[Step 2] Fetching physical truth from InfluxDB")
        
        if not claims_json:
            logger.warning("[Step 2] No claims data, skipping.")
            return None
        
        # ååºåˆ—åŒ–ï¼ˆä½¿ç”¨ StringIO å› ä¸º claims_json æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯æ–‡ä»¶è·¯å¾„ï¼‰
        df = pl.read_json(StringIO(claims_json))
        logger.info(f"[Step 2] Processing {len(df)} claims")
        
        # ğŸ” è°ƒè¯•ï¼šæ‰“å° InfluxDB é…ç½®ï¼ˆä¸åŒ…æ‹¬å®Œæ•´ Tokenï¼‰
        logger.info(f"[DEBUG] INFLUX_URL={INFLUX_URL}")
        logger.info(f"[DEBUG] INFLUX_ORG={INFLUX_ORG}")
        logger.info(f"[DEBUG] INFLUX_BUCKET={INFLUX_BUCKET}")
        logger.info(f"[DEBUG] INFLUX_TOKEN length={len(INFLUX_TOKEN)}, first 10 chars={INFLUX_TOKEN[:10]}")
        
        # åˆå§‹åŒ– InfluxDB å®¢æˆ·ç«¯
        with InfluxClient(
            url=INFLUX_URL,
            token=INFLUX_TOKEN,
            org=INFLUX_ORG,
            bucket=INFLUX_BUCKET,
        ) as influx_client:
            
            # æ„å»ºæŸ¥è¯¢åˆ—è¡¨
            queries = [
                (row["vehicle_id"], row["unix_timestamp"])
                for row in df.iter_rows(named=True)
            ]
            
            # æ‰¹é‡æŸ¥è¯¢
            results = influx_client.query_batch(queries, window_seconds=1)
            
            # æå–ç»“æœ
            actual_x = []
            actual_y = []
            actual_speed = []
            
            for result in results:
                if result:
                    actual_x.append(result.get("actual_x"))
                    actual_y.append(result.get("actual_y"))
                    actual_speed.append(result.get("actual_speed"))
                else:
                    actual_x.append(None)
                    actual_y.append(None)
                    actual_speed.append(None)
            
            # è¿½åŠ åˆ—
            df = df.with_columns([
                pl.Series("actual_x", actual_x, dtype=pl.Float64),
                pl.Series("actual_y", actual_y, dtype=pl.Float64),
                pl.Series("actual_speed", actual_speed, dtype=pl.Float64),
            ])
        
        logger.info(f"[Step 2] Fetched physical truth for {len(df)} records")
        
        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°æœ‰ InfluxDB æ•°æ®çš„æ ·æœ¬è®°å½•
        df_with_data = df.filter(pl.col("actual_x").is_not_null())
        logger.info(f"[Step 2] Records with InfluxDB data: {len(df_with_data)} / {len(df)}")
        if len(df_with_data) > 0:
            logger.info(f"[DEBUG] Sample records with InfluxDB data (first 5):")
            logger.info(f"\n{df_with_data.head(5)}")
        
        return df.write_json()

    @task
    def fetch_semantic_truth(df_json: Optional[str]) -> Optional[str]:
        """
        Step 3: ä»åœ°å›¾æœåŠ¡è·å–è¯­ä¹‰å±‚çœŸç›¸ï¼ˆé“è·¯ç±»å‹ï¼‰
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. ååºåˆ—åŒ– Step 2 çš„ DataFrame
        2. æŒ‰ vehicle_id åˆ†ç»„ï¼Œæ„å»ºæ‰¹é‡è¯·æ±‚
        3. è°ƒç”¨ Map APIï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰
        4. å°† map_road_type è¿½åŠ åˆ° DataFrame
        
        Args:
            df_json: Step 2 è¾“å‡ºçš„ JSON å­—ç¬¦ä¸²
        
        Returns:
            str: æ›´æ–°åçš„ DataFrame (JSON)
        """
        logger = logging.getLogger("airflow.task")
        logger.info("[Step 3] Fetching semantic truth from Map API")
        
        if not df_json:
            logger.warning("[Step 3] No data from Step 2, skipping.")
            return None
        
        # ååºåˆ—åŒ–ï¼ˆä½¿ç”¨ StringIO å› ä¸º df_json æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯æ–‡ä»¶è·¯å¾„ï¼‰
        df = pl.read_json(StringIO(df_json))
        logger.info(f"[Step 3] Processing {len(df)} records")
        
        # åˆå§‹åŒ– Map å®¢æˆ·ç«¯
        map_client = MapClient(
            base_url=MAP_API_URL,
            port=MAP_PORT,
        )
        
        # æŒ‰ vehicle_id åˆ†ç»„ï¼Œæ„å»ºæ‰¹é‡è¯·æ±‚
        # æ ¼å¼: {vehicle_id: [{"x": float, "y": float, "timestamp": int}, ...]}
        vehicle_points = {}
        
        for row in df.iter_rows(named=True):
            vehicle_id = row["vehicle_id"]
            
            # è·³è¿‡ç¼ºå¤±åæ ‡çš„è®°å½•
            if row["actual_x"] is None or row["actual_y"] is None:
                continue
            
            if vehicle_id not in vehicle_points:
                vehicle_points[vehicle_id] = []
            
            vehicle_points[vehicle_id].append({
                "x": row["actual_x"],
                "y": row["actual_y"],
                "timestamp": row["unix_timestamp"],
            })
        
        logger.info(f"[Step 3] Grouped into {len(vehicle_points)} vehicles for batch query")
        
        # æ‰¹é‡æŸ¥è¯¢
        map_results = map_client.annotate_multiple_vehicles(vehicle_points)
        
        # å°†ç»“æœæ˜ å°„å› DataFrame
        # æ„å»º (vehicle_id, unix_timestamp) -> road_type çš„å­—å…¸
        road_type_map = {}
        for vehicle_id, points in vehicle_points.items():
            road_types = map_results.get(vehicle_id, [])
            for i, point in enumerate(points):
                timestamp = point["timestamp"]
                road_type = road_types[i] if i < len(road_types) else None
                road_type_map[(vehicle_id, timestamp)] = road_type
        
        # è¿½åŠ åˆ° DataFrame
        map_road_types = [
            road_type_map.get((row["vehicle_id"], row["unix_timestamp"]))
            for row in df.iter_rows(named=True)
        ]
        
        df = df.with_columns(
            pl.Series("map_road_type", map_road_types, dtype=pl.Utf8)
        )
        
        logger.info(f"[Step 3] Fetched semantic truth for {len(df)} records")
        
        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°æœ‰ Map API æ•°æ®çš„æ ·æœ¬è®°å½•
        df_with_map = df.filter(pl.col("map_road_type").is_not_null())
        logger.info(f"[Step 3] Records with Map API data: {len(df_with_map)} / {len(df)}")
        if len(df_with_map) > 0:
            logger.info(f"[DEBUG] Sample records with Map API results (first 5):")
            logger.info(f"\n{df_with_map.head(5)}")
        
        return df.write_json()

    @task
    def validate_and_persist(df_json: Optional[str], **context) -> None:
        """
        Step 4: ä½¿ç”¨ Pandera éªŒè¯è§„åˆ™å¹¶å†™å…¥ç»“æœè¡¨
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. ååºåˆ—åŒ– Step 3 çš„ DataFrame
        2. å®šä¹‰ Pandera Schemaï¼ˆéªŒè¯è§„åˆ™ï¼‰ï¼š
           - Location: map_road_type MUST contain "QC"
           - Stationarity: actual_speed <= 0.5
        3. æ‰§è¡ŒéªŒè¯
        4. ç»Ÿè®¡ Pass/Fail
        5. å†™å…¥ç»“æœè¡¨ï¼ˆåŒ…å« shift_date, vehicle_idï¼‰
        
        Args:
            df_json: Step 3 è¾“å‡ºçš„ JSON å­—ç¬¦ä¸²
            shift_date: ç­æ¬¡æ—¥æœŸï¼ˆç”¨äºå†™å…¥ç»“æœè¡¨ï¼‰
            vehicle_ids: è½¦è¾† ID åˆ—è¡¨ï¼ˆç”¨äºå†™å…¥ç»“æœè¡¨ï¼‰
        """
        logger = logging.getLogger("airflow.task")
        logger.info("[Step 4] Validating and persisting results")
        
        # ä» dag_run.conf è¯»å–å‚æ•°
        dag_run = context['dag_run']
        shift_date = dag_run.conf.get('target_shift_date', '')
        vehicle_ids = dag_run.conf.get('vehicle_list', [])
        
        if not df_json:
            logger.warning("[Step 4] No data from Step 3, skipping.")
            # å†™å…¥ç©ºç»“æœï¼ˆé¿å… Controller å¡æ­»ï¼‰
            _write_empty_results(shift_date, vehicle_ids)
            return
        
        # ååºåˆ—åŒ–ï¼ˆä½¿ç”¨ StringIO å› ä¸º df_json æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯æ–‡ä»¶è·¯å¾„ï¼‰
        df_polars = pl.read_json(StringIO(df_json))
        logger.info(f"[Step 4] Validating {len(df_polars)} records")
        
        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°è¿›å…¥éªŒè¯é˜¶æ®µçš„æ ·æœ¬æ•°æ®
        logger.info(f"[DEBUG] Sample records entering validation (first 5):")
        logger.info(f"\n{df_polars.head(5)}")
        
        # å°† Polars DataFrame è½¬æ¢ä¸º Pandas DataFrameï¼ˆPandera 0.19.3 åªæ”¯æŒ Pandasï¼‰
        df = df_polars.to_pandas()
        logger.info(f"[DEBUG] Converted to Pandas DataFrame for validation")
        
        # å®šä¹‰ Pandera Schema
        schema = DataFrameSchema(
            {
                "vehicle_id": Column(str, nullable=False),
                "cycle_id": Column(str, nullable=True),
                "unix_timestamp": Column(int, nullable=False),
                "actual_x": Column(float, nullable=True),
                "actual_y": Column(float, nullable=True),
                "actual_speed": Column(
                    float,
                    nullable=True,
                    checks=[
                        # é™æ­¢æ€§æ£€æŸ¥ï¼šactual_speed <= 0.5ï¼ˆä»…å¯¹é null å€¼æ£€æŸ¥ï¼‰
                        Check.less_than_or_equal_to(MAX_SPEED_FOR_STATIONARY, name="stationarity_check"),
                    ],
                ),
                "map_road_type": Column(
                    str,
                    nullable=True,
                    checks=[
                        Check.str_contains("QC", name="location_must_be_qc"),
                    ],
                ),
            },
            strict=False,
        )
        
        # æ‰§è¡ŒéªŒè¯ï¼ˆæ•è·å¤±è´¥è®°å½•ï¼‰
        failed_indices_set = set()
        try:
            validated_df = schema.validate(df, lazy=True)
            logger.info(f"[Step 4] Validation passed for {len(validated_df)} records")
            
        except pa.errors.SchemaErrors as e:
            logger.warning(f"[Step 4] Validation failed: {e}")
            
            # è§£æå¤±è´¥è®°å½•
            failure_cases = e.failure_cases
            logger.info(f"[Step 4] Failure cases summary:\n{failure_cases}")
            
            # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºå¤±è´¥è®°å½•çš„è¯¦ç»†ä¿¡æ¯å¹¶å†™å…¥æ˜ç»†è¡¨
            if failure_cases is not None and len(failure_cases) > 0:
                # è·å–å¤±è´¥çš„è¡Œç´¢å¼•
                if "index" in failure_cases.columns:
                    failed_indices_set = set(failure_cases["index"].unique().tolist())
                    if failed_indices_set:
                        df_failed = df.loc[df.index.isin(failed_indices_set)]
                        logger.warning(f"[DEBUG] Failed records details (first 10):")
                        logger.warning(f"\n{df_failed.head(10)}")
                        
                        # å†™å…¥å¤±è´¥æ˜ç»†è¡¨
                        _write_failure_details(shift_date, df_failed, failure_cases)
        
        # æŒ‰ vehicle_id åˆ†ç»„ç»Ÿè®¡
        import pandas as pd
        vehicle_stats = {}
        
        for vehicle_id in df['vehicle_id'].unique():
            df_vehicle = df[df['vehicle_id'] == vehicle_id]
            total_vehicle = len(df_vehicle)
            
            # è®¡ç®—è¯¥è½¦è¾†çš„å¤±è´¥è®°å½•æ•°
            vehicle_failed_count = len([idx for idx in df_vehicle.index if idx in failed_indices_set])
            vehicle_passed_count = total_vehicle - vehicle_failed_count
            
            vehicle_stats[vehicle_id] = {
                'total': total_vehicle,
                'passed': vehicle_passed_count,
                'failed': vehicle_failed_count
            }
            
            logger.info(f"[Step 4] Vehicle {vehicle_id}: Total={total_vehicle}, Passed={vehicle_passed_count}, Failed={vehicle_failed_count}")
        
        # æ€»ä½“ç»Ÿè®¡ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        total_all = len(df)
        failed_all = len(failed_indices_set)
        passed_all = total_all - failed_all
        logger.info(f"[Step 4] Overall summary: Total={total_all}, Passed={passed_all}, Failed={failed_all}")
        
        # å†™å…¥ç»“æœè¡¨ï¼ˆæŒ‰è½¦è¾†ï¼‰
        _write_results(shift_date, vehicle_stats)

    def _write_empty_results(shift_date: str, vehicle_ids: List[str]) -> None:
        """å†™å…¥ç©ºç»“æœï¼ˆé¿å… Controller å¡æ­»ï¼‰"""
        logger = logging.getLogger("airflow.task")
        hook = MySqlHook(mysql_conn_id=CONN_ID_DATALOG)
        
        for vehicle_id in vehicle_ids:
            sql = f"""
            INSERT INTO {TBL_RESULT} 
            (shift_date, vehicle_id, total_records, passed_records, failed_records, created_at)
            VALUES ('{shift_date}', '{vehicle_id}', 0, 0, 0, NOW())
            ON DUPLICATE KEY UPDATE
                total_records = 0,
                passed_records = 0,
                failed_records = 0,
                created_at = NOW()
            """
            hook.run(sql)
        
        logger.info(f"[Step 4] Written empty results for {len(vehicle_ids)} vehicles")

    def _write_results(shift_date: str, vehicle_stats: Dict[str, Dict[str, int]]) -> None:
        """
        å†™å…¥éªŒè¯ç»“æœåˆ° MySQLï¼ˆæŒ‰è½¦è¾†åˆ†ç»„ï¼‰
        
        Args:
            shift_date: ç­æ¬¡æ—¥æœŸ
            vehicle_stats: è½¦è¾†ç»Ÿè®¡å­—å…¸ï¼Œæ ¼å¼ï¼š
                {
                    'AT01': {'total': 10, 'passed': 8, 'failed': 2},
                    'AT02': {'total': 15, 'passed': 15, 'failed': 0},
                    ...
                }
        """
        logger = logging.getLogger("airflow.task")
        hook = MySqlHook(mysql_conn_id=CONN_ID_DATALOG)
        
        for vehicle_id, stats in vehicle_stats.items():
            total = stats['total']
            passed = stats['passed']
            failed = stats['failed']
            
            sql = f"""
            INSERT INTO {TBL_RESULT} 
            (shift_date, vehicle_id, total_records, passed_records, failed_records, created_at)
            VALUES ('{shift_date}', '{vehicle_id}', {total}, {passed}, {failed}, NOW())
            ON DUPLICATE KEY UPDATE
                total_records = {total},
                passed_records = {passed},
                failed_records = {failed},
                updated_at = NOW()
            """
            hook.run(sql)
        
        logger.info(f"[Step 4] Written results for {len(vehicle_stats)} vehicles")

    def _write_failure_details(shift_date: str, df_failed, failure_cases) -> None:
        """å†™å…¥éªŒè¯å¤±è´¥çš„æ˜ç»†è®°å½•åˆ° MySQL"""
        import pandas as pd
        logger = logging.getLogger("airflow.task")
        hook = MySqlHook(mysql_conn_id=CONN_ID_DATALOG)
        
        # åˆå¹¶å¤±è´¥è®°å½•å’Œå¤±è´¥åŸå› 
        # failure_cases åŒ…å«ï¼šindex, column, check, failure_case
        # df_failed åŒ…å«ï¼švehicle_id, cycle_id, unix_timestamp, actual_x, actual_y, actual_speed, map_road_type
        
        # ä¸º df_failed æ·»åŠ ç´¢å¼•ä½œä¸ºåˆ—ï¼ˆç”¨äº mergeï¼‰
        df_failed_with_idx = df_failed.copy()
        df_failed_with_idx['_idx'] = df_failed_with_idx.index
        
        # ä¸º failure_cases æ·»åŠ ç´¢å¼•åˆ—
        failure_cases_with_idx = failure_cases.copy()
        failure_cases_with_idx.rename(columns={'index': '_idx'}, inplace=True)
        
        # åˆå¹¶
        df_merged = pd.merge(
            df_failed_with_idx,
            failure_cases_with_idx[['_idx', 'column', 'check']],
            on='_idx',
            how='left'
        )
        
        logger.info(f"[Step 4] Writing {len(df_merged)} failure detail records")
        
        # é€è¡Œå†™å…¥æ˜ç»†è¡¨
        for _, row in df_merged.iterrows():
            # åˆ¤æ–­å¤±è´¥ç±»å‹å’ŒæœŸæœ›æ¡ä»¶
            check_name = row.get('check', 'unknown')
            column_name = row.get('column', 'unknown')
            
            if check_name == 'location_must_be_qc':
                failure_type = 'road_type_mismatch'
                expected_condition = 'map_road_type åº”åŒ…å« "QC"'
                actual_value = str(row.get('map_road_type', 'NULL'))
            elif check_name == 'stationarity_check':
                failure_type = 'speed_violation'
                expected_condition = f'actual_speed åº” <= {MAX_SPEED_FOR_STATIONARY}'
                actual_value = str(row.get('actual_speed', 'NULL'))
            else:
                failure_type = f'unknown_{column_name}'
                expected_condition = f'{column_name} éªŒè¯å¤±è´¥ï¼ˆ{check_name}ï¼‰'
                actual_value = str(row.get(column_name, 'NULL'))
            
            # SQL è½¬ä¹‰å¤„ç†
            def escape_sql(value):
                if value is None or (isinstance(value, float) and pd.isna(value)):
                    return 'NULL'
                return f"'{str(value).replace(chr(39), chr(39)+chr(39))}'"  # å•å¼•å·è½¬ä¹‰
            
            vehicle_id = escape_sql(row.get('vehicle_id'))
            cycle_id = escape_sql(row.get('cycle_id'))
            unix_timestamp = row.get('unix_timestamp', 0)
            actual_x = row.get('actual_x') if pd.notna(row.get('actual_x')) else None
            actual_y = row.get('actual_y') if pd.notna(row.get('actual_y')) else None
            actual_speed = row.get('actual_speed') if pd.notna(row.get('actual_speed')) else None
            map_road_type = escape_sql(row.get('map_road_type'))
            
            sql = f"""
            INSERT INTO {TBL_FAILURE_DETAIL}
            (shift_date, vehicle_id, cycle_id, unix_timestamp, 
             failure_type, expected_condition, actual_value,
             actual_x, actual_y, actual_speed, map_road_type, created_at)
            VALUES (
                '{shift_date}', 
                {vehicle_id}, 
                {cycle_id}, 
                {unix_timestamp},
                '{failure_type}',
                {escape_sql(expected_condition)},
                {escape_sql(actual_value)},
                {actual_x if actual_x is not None else 'NULL'},
                {actual_y if actual_y is not None else 'NULL'},
                {actual_speed if actual_speed is not None else 'NULL'},
                {map_road_type},
                NOW()
            )
            """
            
            try:
                hook.run(sql)
            except Exception as e:
                logger.error(f"[Step 4] Failed to write failure detail: {e}")
                logger.error(f"[Step 4] SQL: {sql}")
        
        logger.info(f"[Step 4] Finished writing failure details")

    # DAG ä»»åŠ¡æµ
    # å››æ­¥éªŒè¯æµç¨‹ï¼ˆå‚æ•°åœ¨ Task å†…éƒ¨ä» context è¯»å–ï¼‰
    claims_json = extract_claims()
    physical_json = fetch_physical_truth(claims_json)
    semantic_json = fetch_semantic_truth(physical_json)
    validate_and_persist(semantic_json)

