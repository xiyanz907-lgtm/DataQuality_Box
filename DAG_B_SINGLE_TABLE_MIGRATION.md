# DAG B å•è¡¨æ–¹æ¡ˆè¿ç§»æŒ‡å—

## ğŸ“‹ è¿ç§»æ¦‚è¿°

**è¿ç§»æ—¥æœŸ**: 2026-02-02  
**ç‰ˆæœ¬**: v2.0 (å•è¡¨æ–¹æ¡ˆ)  
**è¿ç§»ç±»å‹**: æ¶æ„ä¼˜åŒ–ï¼ˆåŒè¡¨ â†’ å•è¡¨ï¼‰

---

## ğŸ¯ è¿ç§»ç›®æ ‡

å°†èµ„äº§æ‰“åŒ…åŠŸèƒ½ä» **åŒè¡¨æ–¹æ¡ˆ** è¿ç§»åˆ° **å•è¡¨æ–¹æ¡ˆ**ï¼š
- **åŒè¡¨æ–¹æ¡ˆ**: `governance_asset_packing_queue` (é˜Ÿåˆ—) + `auto_test_case_catalog` (meta)
- **å•è¡¨æ–¹æ¡ˆ**: `auto_test_case_catalog` (åˆå¹¶)

---

## âœ… å˜æ›´æ¸…å•

### 1. æ•°æ®åº“å˜æ›´

| æ“ä½œ | è¯¦æƒ… | æ–‡ä»¶ |
|-----|------|------|
| **è¡¨å‡çº§** | `auto_test_case_catalog` æ·»åŠ  9 ä¸ªæ‰“åŒ…å­—æ®µ | `database/schemas/schema_auto_test_case_catalog_v3_single_table.sql` |
| **å­—æ®µæ‰©å±•** | `process_status` æ–°å¢ 4 ä¸ªçŠ¶æ€å€¼ | åŒä¸Š |
| **ç´¢å¼•æ–°å¢** | æ·»åŠ  2 ä¸ªæ‰“åŒ…é˜Ÿåˆ—ç´¢å¼• | åŒä¸Š |
| **è¡¨åºŸå¼ƒ** | `governance_asset_packing_queue` ä¸å†ä½¿ç”¨ | å¯é€‰ï¼šæ‰‹åŠ¨åˆ é™¤ |

### 2. ä»£ç å˜æ›´

| æ–‡ä»¶ | å˜æ›´å†…å®¹ | å˜æ›´ç±»å‹ |
|-----|---------|---------|
| `plugins/datasets.py` | Dataset URI æ”¹ä¸º `auto_test_case_catalog` | ä¿®æ”¹ |
| `dags/governance_main_dag.py` | `save_assets_to_queue` æ”¹ä¸ºå†™å…¥ meta è¡¨ | é‡æ„ |
| `dags/asset_packing_dag.py` | æ‰€æœ‰ SQL æŸ¥è¯¢æ”¹ä¸ºå•è¡¨ | é‡æ„ |
| `database/monitoring/...` | ç›‘æ§ SQL é€‚é…å•è¡¨ | æ–°å»º |

---

## ğŸ“Š æ•°æ®åº“è¡¨ç»“æ„å¯¹æ¯”

### æ–°å¢å­—æ®µï¼ˆauto_test_case_catalogï¼‰

| å­—æ®µå | ç±»å‹ | è¯´æ˜ | å¯¹åº”åŸé˜Ÿåˆ—è¡¨å­—æ®µ |
|-------|------|------|----------------|
| `pack_key` | VARCHAR(200) | å¼‚æ­¥æ‰“åŒ…ä»»åŠ¡Key | âœ… pack_key |
| `pack_url` | VARCHAR(500) | æ‰“åŒ…æ–‡ä»¶URL | âœ… pack_url |
| `pack_base_path` | VARCHAR(500) | æ‰“åŒ…å­˜å‚¨è·¯å¾„ | âœ… base_path |
| `pack_poll_count` | INT | è½®è¯¢æ¬¡æ•° | âœ… poll_count |
| `pack_retry_count` | INT | æ‰“åŒ…é‡è¯•æ¬¡æ•° | âœ… retry_count |
| `pack_error_message` | TEXT | æ‰“åŒ…é”™è¯¯ä¿¡æ¯ | âœ… error_message |
| `pack_started_at` | DATETIME | æ‰“åŒ…å¼€å§‹æ—¶é—´ | âœ… pack_started_at |
| `pack_completed_at` | DATETIME | æ‰“åŒ…å®Œæˆæ—¶é—´ | âœ… processed_at |
| `updated_at` | DATETIME | æ›´æ–°æ—¶é—´ | ğŸ†• æ–°å¢ |

### process_status æ‰©å±•

```sql
-- åŸæœ‰çŠ¶æ€ï¼ˆä¿ç•™ï¼‰
'IDENTIFIED'    -- å·²è¯†åˆ«
'PACKAGED'      -- å·²æ‰“åŒ…
'BENCHMARKED'   -- å·²åŸºå‡†æµ‹è¯•

-- æ–°å¢çŠ¶æ€ï¼ˆé˜Ÿåˆ—ç®¡ç†ï¼‰
'PENDING'       -- å¾…æ‰“åŒ…
'PROCESSING'    -- æ‰“åŒ…ä¸­
'POLLING'       -- è½®è¯¢ä¸­
'ABANDONED'     -- å·²æ”¾å¼ƒ
```

---

## ğŸš€ è¿ç§»æ­¥éª¤

### Step 1: å¤‡ä»½æ•°æ®ï¼ˆå¿…é¡»ï¼‰

```bash
# å¤‡ä»½ auto_test_case_catalog è¡¨
mysqldump -u root -p your_database auto_test_case_catalog > backup_auto_test_case_catalog_$(date +%Y%m%d).sql

# å¤‡ä»½ governance_asset_packing_queue è¡¨ï¼ˆå¦‚æœ‰æ•°æ®ï¼‰
mysqldump -u root -p your_database governance_asset_packing_queue > backup_governance_asset_packing_queue_$(date +%Y%m%d).sql
```

### Step 2: æ‰§è¡Œæ•°æ®åº“å‡çº§

```bash
# è¿›å…¥ MySQL
docker exec -it deploy-mysql-1 mysql -u root -p

# åˆ‡æ¢æ•°æ®åº“
USE your_database_name;

# æ‰§è¡Œå‡çº§è„šæœ¬
SOURCE /path/to/schema_auto_test_case_catalog_v3_single_table.sql;

# éªŒè¯
DESC auto_test_case_catalog;
SHOW INDEX FROM auto_test_case_catalog;
```

### Step 3: æ•°æ®è¿ç§»ï¼ˆå¦‚æœ‰æ—§é˜Ÿåˆ—æ•°æ®ï¼‰

```sql
-- å¯é€‰ï¼šå°†æ—§é˜Ÿåˆ—è¡¨çš„æ•°æ®è¿ç§»åˆ° meta è¡¨
UPDATE auto_test_case_catalog atc
INNER JOIN governance_asset_packing_queue gapq 
    ON atc.id = CAST(gapq.asset_id AS UNSIGNED)
SET 
    atc.process_status = CASE 
        WHEN gapq.status = 'SUCCESS' THEN 'PACKAGED'
        WHEN gapq.status = 'ABANDONED' THEN 'ABANDONED'
        WHEN gapq.status = 'PENDING' THEN 'PENDING'
        WHEN gapq.status = 'PROCESSING' THEN 'PROCESSING'
        WHEN gapq.status = 'POLLING' THEN 'POLLING'
        ELSE atc.process_status
    END,
    atc.pack_key = gapq.pack_key,
    atc.pack_url = gapq.pack_url,
    atc.pack_base_path = gapq.base_path,
    atc.pack_poll_count = gapq.poll_count,
    atc.pack_retry_count = gapq.retry_count,
    atc.pack_error_message = gapq.error_message,
    atc.pack_started_at = gapq.pack_started_at,
    atc.pack_completed_at = gapq.processed_at,
    atc.updated_at = gapq.updated_at
WHERE gapq.status != 'PENDING' OR gapq.retry_count > 0;
```

### Step 4: é‡å¯ Airflow

```bash
cd /home/ubuntu/cactus_box/cactus-box/deploy
docker-compose restart airflow

# ç­‰å¾… 30 ç§’
sleep 30
```

### Step 5: éªŒè¯ DAG

```bash
# æ£€æŸ¥ DAG çŠ¶æ€
docker exec deploy-airflow-1 airflow dags list | grep -E "(governance_main_dag|asset_packing_dag)"

# æ£€æŸ¥ Dataset
docker exec deploy-airflow-1 airflow datasets list | grep auto_test_case_catalog
```

### Step 6: æµ‹è¯•å®Œæ•´æµç¨‹

```bash
# 1. æ‰‹åŠ¨è§¦å‘ DAG A
docker exec deploy-airflow-1 airflow dags trigger governance_main_dag

# 2. æŸ¥çœ‹ DAG A æ—¥å¿—
docker exec deploy-airflow-1 airflow tasks logs governance_main_dag save_assets_to_queue <execution_date>

# 3. éªŒè¯ meta è¡¨çŠ¶æ€
docker exec -i deploy-mysql-1 mysql -u root -p your_database -e \
"SELECT id, cycle_id, process_status, pack_retry_count FROM auto_test_case_catalog WHERE process_status = 'PENDING' LIMIT 5;"

# 4. éªŒè¯ DAG B è‡ªåŠ¨è§¦å‘
docker exec deploy-airflow-1 airflow dags list-runs -d asset_packing_dag --state running

# 5. æŸ¥çœ‹æ‰“åŒ…ç»“æœ
docker exec -i deploy-mysql-1 mysql -u root -p your_database -e \
"SELECT id, cycle_id, process_status, pack_url FROM auto_test_case_catalog WHERE process_status = 'PACKAGED' LIMIT 5;"
```

---

## ğŸ” è¿ç§»éªŒè¯æ¸…å•

### æ•°æ®åº“å±‚é¢
- [ ] `auto_test_case_catalog` è¡¨æ–°å¢ 9 ä¸ªå­—æ®µ
- [ ] `process_status` å­—æ®µæ”¯æŒæ–°çŠ¶æ€å€¼
- [ ] æ–°å¢ 2 ä¸ªæ‰“åŒ…é˜Ÿåˆ—ç´¢å¼•
- [ ] æ—§é˜Ÿåˆ—è¡¨æ•°æ®å·²è¿ç§»ï¼ˆå¦‚æœ‰ï¼‰

### ä»£ç å±‚é¢
- [ ] Dataset URI æŒ‡å‘ `auto_test_case_catalog`
- [ ] DAG A å†™å…¥ meta è¡¨ï¼ŒçŠ¶æ€ä¸º PENDING
- [ ] DAG B æŸ¥è¯¢ meta è¡¨ï¼Œprocess_status = 'PENDING'
- [ ] DAG B æ›´æ–° meta è¡¨ï¼Œprocess_status = 'PACKAGED'

### åŠŸèƒ½å±‚é¢
- [ ] æ‰‹åŠ¨è§¦å‘ DAG A æˆåŠŸ
- [ ] P1 èµ„äº§å†™å…¥ meta è¡¨ï¼ŒçŠ¶æ€ä¸º PENDING
- [ ] Dataset è§¦å‘ DAG B
- [ ] DAG B è·å–å¾…å¤„ç†èµ„äº§
- [ ] æ‰“åŒ…æœåŠ¡è°ƒç”¨æˆåŠŸ
- [ ] çŠ¶æ€æ›´æ–°ä¸º PACKAGED
- [ ] åƒµå°¸ä»»åŠ¡å¤„ç†æ­£å¸¸

---

## ğŸ“ˆ ç›‘æ§é€‚é…

### æ–°ç›‘æ§ SQL

```bash
# ä½¿ç”¨å•è¡¨æ–¹æ¡ˆçš„ç›‘æ§ SQL
cat database/monitoring/asset_packing_monitor_single_table.sql
```

### Grafana é¢æ¿æ›´æ–°

```sql
-- é˜Ÿåˆ—ç§¯å‹
SELECT COUNT(*) FROM auto_test_case_catalog WHERE process_status = 'PENDING'

-- åƒµå°¸ä»»åŠ¡
SELECT COUNT(*) FROM auto_test_case_catalog 
WHERE process_status IN ('PROCESSING', 'POLLING')
  AND updated_at < NOW() - INTERVAL 2 HOUR

-- æ‰“åŒ…æˆåŠŸç‡
SELECT 
    ROUND(SUM(CASE WHEN process_status = 'PACKAGED' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2)
FROM auto_test_case_catalog
WHERE pack_completed_at >= NOW() - INTERVAL 1 HOUR
```

---

## ğŸ”„ å›æ»šæ–¹æ¡ˆ

### å¦‚æœè¿ç§»å¤±è´¥ï¼Œå¯ä»¥å›æ»š

```sql
-- 1. åˆ é™¤æ–°å¢å­—æ®µ
ALTER TABLE auto_test_case_catalog DROP COLUMN pack_key;
ALTER TABLE auto_test_case_catalog DROP COLUMN pack_url;
ALTER TABLE auto_test_case_catalog DROP COLUMN pack_base_path;
ALTER TABLE auto_test_case_catalog DROP COLUMN pack_poll_count;
ALTER TABLE auto_test_case_catalog DROP COLUMN pack_retry_count;
ALTER TABLE auto_test_case_catalog DROP COLUMN pack_error_message;
ALTER TABLE auto_test_case_catalog DROP COLUMN pack_started_at;
ALTER TABLE auto_test_case_catalog DROP COLUMN pack_completed_at;
ALTER TABLE auto_test_case_catalog DROP COLUMN updated_at;

-- 2. åˆ é™¤æ–°å¢ç´¢å¼•
ALTER TABLE auto_test_case_catalog DROP INDEX idx_pack_queue;
ALTER TABLE auto_test_case_catalog DROP INDEX idx_pack_polling;

-- 3. æ¢å¤ä»£ç ï¼ˆGitï¼‰
git checkout <previous_commit_hash>

-- 4. é‡å¯ Airflow
docker-compose restart airflow
```

---

## ğŸ§¹ æ¸…ç†æ—§è¡¨ï¼ˆå¯é€‰ï¼‰

### è¿ç§»æˆåŠŸåï¼Œå¯ä»¥åˆ é™¤æ—§é˜Ÿåˆ—è¡¨

```sql
-- âš ï¸ ç¡®ä¿è¿ç§»å®Œå…¨æˆåŠŸåå†æ‰§è¡Œ

-- 1. éªŒè¯æ—§è¡¨ä¸å†ä½¿ç”¨
SELECT * FROM governance_asset_packing_queue LIMIT 1;

-- 2. å¤‡ä»½æ—§è¡¨ï¼ˆå¦‚éœ€è¦ï¼‰
CREATE TABLE governance_asset_packing_queue_backup AS 
SELECT * FROM governance_asset_packing_queue;

-- 3. åˆ é™¤æ—§è¡¨
DROP TABLE governance_asset_packing_queue;
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | åŒè¡¨æ–¹æ¡ˆ | å•è¡¨æ–¹æ¡ˆ | æ”¹å–„ |
|-----|---------|---------|------|
| **æ•°æ®å†—ä½™** | é«˜ | ä½ | âœ… -50% |
| **å†™å…¥æ¬¡æ•°** | 2 æ¬¡ | 1 æ¬¡ | âœ… -50% |
| **æŸ¥è¯¢å¤æ‚åº¦** | JOIN | å•è¡¨ | âœ… ç®€åŒ– |
| **çŠ¶æ€ä¸€è‡´æ€§** | ä¸­ç­‰ | é«˜ | âœ… å¼ºä¸€è‡´ |
| **ç»´æŠ¤æˆæœ¬** | é«˜ | ä½ | âœ… -40% |

---

## âœ… è¿ç§»å®Œæˆæ£€æŸ¥

### æœ€ç»ˆéªŒè¯

```bash
# 1. æ£€æŸ¥è¡¨ç»“æ„
docker exec -i deploy-mysql-1 mysql -u root -p your_database -e "DESC auto_test_case_catalog;"

# 2. æ£€æŸ¥ DAG çŠ¶æ€
docker exec deploy-airflow-1 airflow dags list | grep governance

# 3. æ£€æŸ¥ Dataset
docker exec deploy-airflow-1 airflow datasets list

# 4. è¿è¡Œå®Œæ•´æµç¨‹æµ‹è¯•
docker exec deploy-airflow-1 airflow dags trigger governance_main_dag

# 5. ç›‘æ§æ—¥å¿—
docker logs -f deploy-airflow-1 | grep -E "(governance_main_dag|asset_packing_dag)"
```

---

## ğŸ“ æ”¯æŒä¸åé¦ˆ

- **é—®é¢˜æŠ¥å‘Š**: data-governance@example.com
- **Slack**: #data-governance-migration
- **æ–‡æ¡£**: [DAG_B_DEPLOYMENT_GUIDE.md](DAG_B_DEPLOYMENT_GUIDE.md)

---

**è¿ç§»çŠ¶æ€**: âœ… ä»£ç å·²å®Œæˆï¼Œç­‰å¾…æ‰§è¡Œ  
**é¢„è®¡è€—æ—¶**: 30 åˆ†é’Ÿï¼ˆå«å¤‡ä»½å’ŒéªŒè¯ï¼‰  
**é£é™©ç­‰çº§**: ğŸŸ¢ ä½ï¼ˆæ”¯æŒå›æ»šï¼‰
