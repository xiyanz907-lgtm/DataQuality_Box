import os
import sys
import pendulum
from airflow.decorators import dag, task
# from airflow.operators.email import EmailOperator
from airflow.utils.trigger_rule import TriggerRule

# ç¡®ä¿ plugins è·¯å¾„åœ¨ sys.path ä¸­
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
plugins_dir = os.path.join(project_root, "plugins")
if plugins_dir not in sys.path:
    sys.path.append(plugins_dir)

from services.logic_runner import LogicRunner

BATCH_SIZE = int(os.getenv("BATCH_SIZE", 50000))

@dag(
    dag_id="worker_cycle_check",
    schedule=None,
    start_date=pendulum.today("UTC").add(days=-1),
    catchup=False,
    max_active_runs=1,
    tags=["kpi", "ngen", "sharding"],
    default_args={"owner": "box_admin"},
)
def worker_cycle_check():

    @task
    def get_batches(**context):
        """
        ä»»åŠ¡ 1: ç¡®å®šç”¨äºå¤„ç†çš„ ID æ‰¹æ¬¡ã€‚
        è¿”å›åŒ…å« 'id_range' çš„å­—å…¸åˆ—è¡¨ã€‚
        æ”¯æŒä¸¤ç§æ¨¡å¼:
        1. ID Range Mode (Source A Trigger): ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ ID èŒƒå›´
        2. Date Mode (Source B Trigger / Manual): æ ¹æ®æ—¥æœŸæŸ¥è¯¢ ID èŒƒå›´
        """
        conf = context["dag_run"].conf
        mode = conf.get("mode")
        batches = []

        # --- æ¨¡å¼ 1: ID Range Mode ---
        if mode == "id_range":
            start_id = conf.get("start_id")
            end_id = conf.get("end_id")
            print(f"ğŸš€ Running in ID Range Mode: {start_id} - {end_id}")
            
            if start_id is not None and end_id is not None:
                current_start = int(start_id)
                final_end = int(end_id)
                while current_start <= final_end:
                    current_end = min(current_start + BATCH_SIZE - 1, final_end)
                    batches.append({"id_range": (current_start, current_end)})
                    current_start += BATCH_SIZE
            else:
                print("âŒ ID Range mode missing start_id or end_id")
            
            print(f"Generated {len(batches)} batches from ID range.")
            return batches

        # --- æ¨¡å¼ 2: Date Mode ---
        target_date = conf.get("date_filter", context["ds"])
        print(f"ğŸ“… Running in Date Mode: {target_date}")
        table_name = "cnt_cycles"
        
        runner = LogicRunner(
            cactus_conn_id="cactus_mysql_conn",
            ngen_conn_id="ngen_mysql_conn"
        )
        
        # å°è¯•è·å– ID è¾¹ç•Œ
        min_id, max_id = runner.get_id_boundaries(table_name, target_date)
        
        if min_id is not None and max_id is not None:
            print(f"Found ID boundaries: {min_id} - {max_id}. Generating shards...")
            current_start = min_id
            while current_start <= max_id:
                current_end = min(current_start + BATCH_SIZE - 1, max_id)
                batches.append({"id_range": (current_start, current_end)})
                current_start += BATCH_SIZE
        else:
            # å•è¡¨æ¨¡å¼æˆ–æœªæ‰¾åˆ°è¾¹ç•Œï¼ˆå¦‚æºè¡¨ä¸ºç©ºï¼‰çš„å…œåº•é€»è¾‘
            # LogicRunner å°†ä½¿ç”¨æ—¥æœŸè¿‡æ»¤æ¥å¤„ç† id_range ä¸º None çš„æƒ…å†µ
            print("No ID boundaries found or single-table mode. Using full date range.")
            batches.append({"id_range": None})
            
        print(f"Generated {len(batches)} batches.")
        return batches

    @task
    def run_check_shard(shard_config, **context):
        """
        ä»»åŠ¡ 2: å¯¹ç‰¹å®šåˆ†ç‰‡è¿è¡Œæ£€æŸ¥ã€‚
        """
        target_date = context["dag_run"].conf.get("date_filter", context["ds"])
        table_name = "cnt_cycles"
        id_range = shard_config.get("id_range")
        
        runner = LogicRunner(
            cactus_conn_id="cactus_mysql_conn",
            ngen_conn_id="ngen_mysql_conn"
        )
        
        result = runner.run_checks(
            table_name=table_name,
            date_filter=target_date,
            id_range=id_range
        )
        
        # å°† id_range æ³¨å…¥ç»“æœä»¥ä¾¿ debug
        result["shard_info"] = str(id_range)
        return result

    @task
    def summarize_results(results, **context):
        """
        ä»»åŠ¡ 3: èšåˆæ‰€æœ‰åˆ†ç‰‡çš„ç»“æœã€‚
        ä½¿ç”¨ ReportGenerator ç”Ÿæˆç»Ÿä¸€çš„ HTML æŠ¥å‘Š
        """
        from services.report_generator import ReportGenerator

        # æ˜¾å¼å°† LazyXComAccess è½¬æ¢ä¸º list
        results = list(results)
        
        # è°ƒç”¨é€šç”¨ç”Ÿæˆå™¨
        summary_dict = ReportGenerator.generate_html_report(results, title="Cactus CycleCheck è´¨é‡æ£€æµ‹æŠ¥å‘Š")
        
        print(summary_dict["report_text"])
        
        # å­˜å…¥ XCom
        context["ti"].xcom_push(key="qa_result", value=summary_dict)

        # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿å°†ä»»åŠ¡çŠ¶æ€æ ‡è®°ä¸º failedï¼Œä¾› global_alert_reporter è½®è¯¢
        if summary_dict.get("status") == "FAILED":
             raise ValueError(f"Data Quality Checks Failed: {summary_dict.get('violation_count')} violations found.")

        return summary_dict

    # å®šä¹‰ DAG ç»“æ„
    batches = get_batches()
    results = run_check_shard.expand(shard_config=batches)
    summary = summarize_results(results)
    
    # ä»»åŠ¡ 4: é‚®ä»¶é€šçŸ¥ (å·²ç§»è‡³ global_alert_reporter)
    # recipients = os.getenv("ALERT_EMAIL_TO", "xiyan.zhou@westwell-lab.com")
    # send_email = EmailOperator(
    #     task_id="send_report_email",
    #     to=recipients,
    #     subject='[Quality] Cactus æ•°æ®è´¨é‡æ£€æµ‹æŠ¥å‘Š ({{ dag_run.conf.get("date_filter", ds) }})',
    #     html_content="""
    #     {% set r = task_instance.xcom_pull(task_ids='summarize_results', key='qa_result') or {} %}
    #     {{ r.get('html_report', 'Error generating report') }}
    #     """,
    #     trigger_rule=TriggerRule.ALL_DONE,
    # )

    # summary >> send_email

# å®ä¾‹åŒ– DAG
worker_cycle_check()
